{
    "version": "https://jsonfeed.org/version/1",
    "title": "EA1HET / 0x30.io",
    "description": "",
    "home_page_url": "https://0x30.io",
    "feed_url": "https://0x30.io/feed.json",
    "user_comment": "",
    "author": {
        "name": "Jonathan Gonzalez"
    },
    "items": [
        {
            "id": "https://0x30.io/the-esp8266-hardware/",
            "url": "https://0x30.io/the-esp8266-hardware/",
            "title": "The ESP8266 hardware",
            "summary": "While the ESP8266 is often used as a ‘dumb’ Serial-to-WiFi bridge, it’s a very powerful&hellip;",
            "content_html": "<p>While the ESP8266 is often used as a ‘dumb’ Serial-to-WiFi bridge, it’s a very powerful microcontroller on its own. In this chapter, we’ll look at the non-Wi-Fi specific functions of the ESP8266.</p>\n<h3 id=\"mcetoc_1di7poe710\">Digital I/O</h3>\n<p>Just like a normal Arduino, the ESP8266 has digital input/output pins (I/O or GPIO, General Purpose Input/Output pins). As the name implies, they can be used as digital inputs to read a digital voltage, or as digital outputs to output either 0V (sink current) or 3.3V (source current).</p>\n<h4 id=\"mcetoc_1di7poe721\">Voltage and current restrictions</h4>\n<p>The ESP8266 is a 3.3V microcontroller, so its I/O operates at 3.3V as well. The pins are <strong>not 5V tolerant, applying more than 3.6V on any pin will kill the chip.</strong></p>\n<p>The maximum current that can be drawn from a single GPIO pin is <strong>12mA</strong>.</p>\n<h4 id=\"mcetoc_1di7poe722\">Usable pins</h4>\n<p>The ESP8266 has 17 GPIO pins (0-16), however, you can only use 11 of them, because 6 pins (GPIO 6 - 11) are used to connect the flash memory chip. This is the small 8-legged chip right next to the ESP8266. If you try to use one of these pins, you might crash your program.</p>\n<p>GPIO 1 and 3 are used as TX and RX of the hardware Serial port (UART), so in most cases, you can’t use them as normal I/O while sending/receiving serial data.</p>\n<h4 id=\"mcetoc_1di7poe723\">Boot modes</h4>\n<p>As mentioned in the previous chapter, some I/O pins have a special function during boot: They select 1 of 3 boot modes:</p>\n<table>\n<thead>\n<tr>\n<th>GPIO15</th>\n<th>GPIO0</th>\n<th>GPIO2</th>\n<th>Mode</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0V</td>\n<td>0V</td>\n<td>3.3V</td>\n<td>Uart Bootloader</td>\n</tr>\n<tr>\n<td>0V</td>\n<td>3.3V</td>\n<td>3.3V</td>\n<td>Boot sketch (SPI flash)</td>\n</tr>\n<tr>\n<td>3.3V</td>\n<td>x</td>\n<td>x</td>\n<td>SDIO mode (not used for Arduino)</td>\n</tr>\n</tbody>\n</table>\n<p>Note: you don’t have to add an external pull-up resistor to GPIO2, the internal one is enabled at boot.</p>\n<p>We made sure that these conditions are met by adding external resistors in the previous chapter, or the board manufacturer of your board added them for you. This has some implications, however:</p>\n<ul>\n<li>GPIO15 is always pulled low, so you can’t use the internal pull-up resistor. You have to keep this in mind when using GPIO15 as an input to read a switch or connect it to a device with an open-collector (or open-drain) output, like I²C.</li>\n<li>GPIO0 is pulled high during normal operation, so you can’t use it as a Hi-Z input.</li>\n<li>GPIO2 can’t be low at boot, so you can’t connect a switch to it.</li>\n</ul>\n<h4 id=\"mcetoc_1di7poe724\">Internal pull-up/-down resistors</h4>\n<p>GPIO 0-15 all have a built-in pull-up resistor, just like in an Arduino. GPIO16 has a built-in pull-down resistor.</p>\n<h4 id=\"mcetoc_1di7poe725\">PWM</h4>\n<p>Unlike most Atmel chips (Arduino), the ESP8266 doesn’t support hardware PWM, however, software PWM is supported on all digital pins. The default PWM range is 10-bits @ 1kHz, but this can be changed (up to &gt;14-bit@1kHz).</p>\n<h3 id=\"mcetoc_1di7poe726\">Analog input</h3>\n<p>The ESP8266 has a single analog input, with an input range of 0 - 1.0V. If you supply 3.3V, for example, you will damage the chip. Some boards like the NodeMCU have an on-board resistive voltage divider, to get an easier 0 - 3.3V range. You could also just use a trimpot as a voltage divider.</p>\n<p>The ADC (analog to digital converter) has a resolution of 10 bits.</p>\n<h3 id=\"mcetoc_1di7poe727\">Communication</h3>\n<h4 id=\"mcetoc_1di7poe728\">Serial</h4>\n<p>The ESP8266 has two hardware UARTS (Serial ports):<br>UART0 on pins 1 and 3 (TX0 and RX0 resp.), and UART1 on pins 2 and 8 (TX1 and RX1 resp.), however, GPIO8 is used to connect the flash chip. This means that UART1 can only transmit data.</p>\n<p>UART0 also has hardware flow control on pins 15 and 13 (RTS0 and CTS0 resp.). These two pins can also be used as alternative TX0 and RX0 pins.</p>\n<h4 id=\"mcetoc_1di7poe729\">I²C</h4>\n<p>The ESP doesn’t have a hardware TWI (Two Wire Interface), but it is implemented in software. This means that you can use pretty much any two digital pins. By default, the I²C library uses pin 4 as SDA and pin 5 as SCL. (The data sheet specifies GPIO2 as SDA and GPIO14 as SCL.) The maximum speed is approximately 450kHz.</p>\n<h4 id=\"mcetoc_1di7poe72a\">SPI</h4>\n<p>The ESP8266 has one SPI connection available to the user, referred to as HSPI. It uses GPIO14 as CLK, 12 as MISO, 13 as MOSI and 15 as Slave Select (SS). It can be used in both Slave and Master mode (in software).</p>\n<h3 id=\"mcetoc_1di7poe72b\">GPIO overview</h3>\n<table>\n<thead>\n<tr>\n<th>GPIO</th>\n<th>Function</th>\n<th>State</th>\n<th>Restrictions</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>Boot mode select</td>\n<td>3.3V</td>\n<td>No Hi-Z</td>\n</tr>\n<tr>\n<td>1</td>\n<td>TX0</td>\n<td>-</td>\n<td>Not usable during Serial transmission</td>\n</tr>\n<tr>\n<td>2</td>\n<td>Boot mode select<br>TX1</td>\n<td>3.3V (boot only)</td>\n<td>Don’t connect to ground at boot time <br>Sends debug data at boot time</td>\n</tr>\n<tr>\n<td>3</td>\n<td>RX0</td>\n<td>-</td>\n<td>Not usable during Serial transmission</td>\n</tr>\n<tr>\n<td>4</td>\n<td>SDA (I²C)</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>5</td>\n<td>SCL (I²C)</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>6 - 11</td>\n<td>Flash connection</td>\n<td>x</td>\n<td>Not usable, and not broken out</td>\n</tr>\n<tr>\n<td>12</td>\n<td>MISO (SPI)</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>13</td>\n<td>MOSI (SPI)</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>14</td>\n<td>SCK (SPI)</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>15</td>\n<td>SS (SPI)</td>\n<td>0V</td>\n<td>Pull-up resistor not usable</td>\n</tr>\n<tr>\n<td>16</td>\n<td>Wake up from sleep</td>\n<td>-</td>\n<td>No pull-up resistor, but pull-down instead <br>Should be connected to RST to wake up</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"mcetoc_1di7poe72c\">The ESP8266 as a microcontroller - Software</h2>\n<p>Most of the microcontroller functionality of the ESP uses exactly the same syntax as a normal Arduino, making it really easy to get started.</p>\n<h3 id=\"mcetoc_1di7poe72d\">Digital I/O</h3>\n<p>Just like with a regular Arduino, you can set the function of a pin using <code>pinMode(pin, mode);</code> where <code>pin</code> is the GPIO number*, and <code>mode</code> can be either <code>INPUT</code>, which is the default, <code>OUTPUT</code>, or <code>INPUT_PULLUP</code> to enable the built-in pull-up resistors for GPIO 0-15. To enable the pull-down resistor for GPIO16, you have to use <code>INPUT_PULLDOWN_16</code>.</p>\n<p>(*) NodeMCU uses a different pin mapping, read more <a href=\"https://github.com/esp8266/Arduino/blob/master/doc/boards.md#nodemcu-09\">here</a>. To address a NodeMCU pin, e.g. pin 5, use D5: for instance: <code>pinMode(D5, OUTPUT);</code></p>\n<p>To set an output pin high (3.3V) or low (0V), use <code>digitalWrite(pin, value);</code> where <code>pin</code> is the digital pin, and <code>value</code> either 1 or 0 (or <code>HIGH</code> and <code>LOW</code>).</p>\n<p>To read an input, use <code>digitalRead(pin);</code></p>\n<p>To enable PWM on a certain pin, use <code>analogWrite(pin, value);</code> where <code>pin</code> is the digital pin, and <code>value</code> a number between 0 and 1023.</p>\n<p>You can change the range (bit depth) of the PWM output by using <code>analogWriteRange(new_range);</code></p>\n<p>The frequency can be changed by using <code>analogWriteFreq(new_frequency);</code>. <code>new_frequency</code> should be between 100 and 1000Hz.</p>\n<h3 id=\"mcetoc_1di7poe72e\">Analog input</h3>\n<p>Just like on an Arduino, you can use <code>analogRead(A0)</code> to get the analog voltage on the analog input. (0 = 0V, 1023 = 1.0V).</p>\n<p>The ESP can also use the ADC to measure the supply voltage (V<sub>CC</sub>). To do this, include <code>ADC_MODE(ADC_VCC);</code> at the top of your sketch, and use <code>ESP.getVcc();</code> to actually get the voltage.<br>If you use it to read the supply voltage, you can’t connect anything else to the analog pin.</p>\n<h3 id=\"mcetoc_1di7poe72f\">Communication</h3>\n<h4 id=\"mcetoc_1di7poe72g\">Serial communication</h4>\n<p>To use UART0 (TX = GPIO1, RX = GPIO3), you can use the <code>Serial</code> object, just like on an Arduino: <code>Serial.begin(baud)</code>.</p>\n<p> </p>\n<p>To use the alternative pins (TX = GPIO15, RX = GPIO13), use <code>Serial.swap()</code> after <code>Serial.begin</code>.</p>\n<p>To use UART1 (TX = GPIO2), use the <code>Serial1</code> object.</p>\n<p>All Arduino Stream functions, like <code>read, write, print, println, ...</code> are supported as well.</p>\n<h4 id=\"mcetoc_1di7poe72h\">I²C and SPI</h4>\n<p>You can just use the default Arduino library syntax, like you normally would.</p>\n<h3 id=\"mcetoc_1di7poe72i\">Sharing CPU time with the RF part</h3>\n<p>One thing to keep in mind while writing programs for the ESP8266 is that your sketch has to share resources (CPU time and memory) with the Wi-Fi- and TCP-stacks (the software that runs in the background and handles all Wi-Fi and IP connections). <br>If your code takes too long to execute, and don’t let the TCP stacks do their thing, it might crash, or you could lose data. It’s best to keep the execution time of you loop under a couple of hundreds of milliseconds.</p>\n<p>Every time the main loop is repeated, your sketch yields to the Wi-Fi and TCP to handle all Wi-Fi and TCP requests.</p>\n<p>If your loop takes longer than this, you will have to explicitly give CPU time to the Wi-Fi/TCP stacks, by using including <code>delay(0);</code> or <code>yield();</code>. If you don’t, network communication won’t work as expected, and if it’s longer than 3 seconds, the soft WDT (Watch Dog Timer) will reset the ESP. If the soft WDT is disabled, after a little over 8 seconds, the hardware WDT will reset the chip.</p>\n<p>From a microcontroller’s perspective however, 3 seconds is a very long time (240 million clockcycles), so unless you do some extremely heavy number crunching, or sending extremely long strings over Serial, you won’t be affected by this. Just keep in mind that you add the <code>yield();</code> inside your <code>for</code> or <code>while</code> loops that could take longer than, say 100ms.</p>\n<h3 id=\"mcetoc_1di7poe72j\">Sources</h3>\n<p>This is where I got most of my information to writ this article, there’s some more details on the GitHub pages, if you’re into some more advanced stuff, like EEPROM or deep sleep etc.</p>\n<ul>\n<li><a href=\"https://github.com/esp8266/Arduino/issues/2942\">https://github.com/esp8266/Arduino/issues/2942</a></li>\n<li><a href=\"https://github.com/esp8266/Arduino/pull/2533/files\">https://github.com/esp8266/Arduino/pull/2533/files</a></li>\n<li><a href=\"https://github.com/esp8266/Arduino/blob/master/doc/libraries.md\">https://github.com/esp8266/Arduino/blob/master/doc/libraries.md</a></li>\n<li><a href=\"https://github.com/esp8266/Arduino/blob/master/doc/reference.md\">https://github.com/esp8266/Arduino/blob/master/doc/reference.md</a></li>\n<li><a href=\"https://github.com/esp8266/Arduino/blob/master/doc/boards.md\">https://github.com/esp8266/Arduino/blob/master/doc/boards.md</a></li>\n</ul>",
            "image": "https://0x30.io/media/posts/14/kisscc0-esp8266-arduino-wi-fi-input-output-computer-icons-esp01-like-wifi-module-5b748100ac8846.2815384415343618567067.png",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2019-08-14T12:11:48+02:00",
            "date_modified": "2019-08-14T12:13:02+02:00"
        },
        {
            "id": "https://0x30.io/application-security/",
            "url": "https://0x30.io/application-security/",
            "title": "Application Security",
            "summary": "A review on SAST, DAST, IAST and RASPIt’s estimated that 90 percent of security incidents&hellip;",
            "content_html": "<h2 id=\"sast_dast_iast_and_rasp\" class=\"sectionedit2\">A review on SAST, DAST, IAST and RASP</h2>\n<div class=\"level2\">\n<p>It’s estimated that 90 percent of security incidents result from attackers exploiting known software bugs. Needless to say, squashing those bugs in the development phase of software could reduce the information security risks facing many organizations today. To do that, a number of technologies are available to help developers catch security flaws before they’re baked into a final software release. They include SAST, DAST, IAST, and RASP.</p>\n<h2 id=\"mcetoc_1dhl8k5f01\"><strong>SAST vs DAST</strong></h2>\n<p>SAST, or Static Application Security Testing, also known as “white box testing” has been around for more than a decade. It allows developers to find security vulnerabilities in the application source code earlier in the software development life cycle. It also ensures conformance to coding guidelines and standards without actually executing the underlying code.</p>\n<p>DAST, or Dynamic Application Security Testing, also known as “black box” testing, can find security vulnerabilities and weaknesses in a running application, typically web apps. It does that by employing fault injection techniques on an app, such as feeding malicious data to the software, to identify common security vulnerabilities, such as SQL injection and cross-­site scripting. DAST can also cast a spotlight in runtime problems that can’t be identified by static analysis­­ for example, authentication and server configuration issues, as well as flaws visible only when a known user logs in.</p>\n<h2 id=\"mcetoc_1dhl8khlv2\"><strong>SAST &amp; DAST Are Usually Used in Tandem </strong></h2>\n<p>SAST and DAST are often used in tandem because SAST isn’t going to find runtime errors and DAST isn’t going to flag coding errors, at least not down to the code line number. SAST performs well when it comes to finding an error in a line of code, such as weak random number generation, but usually not very efficient in finding data flow flaws. In addition, SAST solutions are notorious for the larger amount of false positive or false negatives. We created reshift, a free static security testing tool that uses our proprietary machine learning algorithm to triage false positives faster, check it out here if you are interested.</p>\n<p><em class=\"u\">Abstract Interpretation:</em> Some success in reducing or entirely eliminating false positives has been achieved with something called Abstract Interpretation. However, to get the best results, abstract interpretation algorithms need to be tailored to codes using an application’s domain, which includes its architecture, how it uses certain numerical algorithms and the types of data structures it manipulates.</p>\n<p><img src=\"https://notes.0x30.io/lib/exe/fetch.php?w=600&amp;tok=66fd04&amp;media=cybersec:dastvssast.jpeg\"></p>\n<div class=\"level2\">\n<p>Despite SAST’s imperfections, it remains a favorite among development teams. They like that it allows them to scan a project at the code level, which makes it easier for individual team members to make the changes recommended by the technology. it also lets them find flaws early in the development process, which helps reduce the costs and ripple effects that result from addressing problems at the end of the process.</p>\n<p>What’s more, SAST can be automated and transparently integrated into a project’s workflow. That removes some of the hassle typically associated with testing apps for security and contrasts sharply with DAST where, for large projects, a special infrastructure needs to be created, special tests performed and multiple instances of an application run in parallel with different input data.</p>\n<p>DAST, though, understands arguments and function calls so it can determine if a call is behaving as it should be. SAST can’t check calls and in most cases, is unable to check argument values.</p>\n</div>\n<h2 id=\"interactive_application_security_testing_iast\" class=\"sectionedit3\">Interactive Application Security Testing (IAST)</h2>\n<div class=\"level2\">\n<p>IAST or Interactive Application Security Testing. Because both SAST and DAST are older technologies, there are those who argue they lack what it takes to secure modern web and mobile apps. For example, SAST has a difficult time dealing with libraries and frameworks found in modern apps. That’s because static tools only see the application source code they can follow. What’s more, libraries and third­party components often cause static tools to choke, producing “lost sources” and “lost sinks” messages. The same is true for frameworks. Run a static tool on an <abbr title=\"Application Programming Interface\">API</abbr>, web service or REST endpoint, and it won’t find anything wrong in them because it can’t understand the framework.</p>\n<p>IAST is designed to address the shortcomings of SAST and DAST by combining elements of both approaches. IAST places an agent within an application and performs all its analysis in the app in real-time and anywhere in the development process ­­ IDE, continuous integrated environment, QA or even in production.</p>\n<p>Because the IAST agent is working inside the app, it can apply its analysis to the entire app ­­ all its code; its runtime control and data flow information; its configuration information; HTTP requests and responses; libraries, frameworks and other components; and backend connection information. Access to all that information allows the IAST engine to cover more code, produce more accurate results and verify a broader range of security rules than either SAST or DAST.</p>\n<figure class=\"post__video\"><iframe width=\"560\" height=\"314\" src=\"https://www.youtube.com/embed/5rxkEz7mjIk\" allowfullscreen=\"allowfullscreen\" ></iframe></figure>\n<p> </p>\n<h2 id=\"run-time_application_security_protection_rasp\" class=\"sectionedit4\">Run-time Application Security Protection (RASP)</h2>\n<div class=\"level2\">\n<p>RASP, or Run-time Application Security Protection As with IAST, RASP, or Run­time Application Security Protection, works inside the application, but it is less a testing tool and more a security tool. It’s plugged into an application or its run­time environment and can control application execution. That allows RASP to protect the app even if a network’s perimeter defenses are breached and the apps contain security vulnerabilities missed by the development team. RASP lets an app run continuous security checks on itself and respond to live attacks by terminating an attacker’s session and alerting defenders to the attack.</p>\n<p>An issue particular to RASP is it can create a sense of false security within a development team. They may not adhere to security best practices thinking, “If we miss something, RASP will pick it up.”</p>\n<p>The problem with technologies like IAST and RASP is they can have an adverse effect on application performance, although boosters of the tech any performance hits are minimal. An issue particular to RASP is it can create a sense of false security within a development team. They may not adhere to security best practices thinking, “If we miss something, RASP will pick it up.” But even if RASP finds a flaw, the development team still has to fix the problem and while they do, the application may have to be taken offline, costing an organization time, money and customer goodwill.</p>\n<p>Regardless of the challenges found in technologies like SAST, DAST, IAST and RASP, using them can create software that’s more secure and do it in a way that’s faster and more cost ­effective than tacking all security testing to the tail of the development process.</p>\n<figure class=\"post__video\"><iframe width=\"560\" height=\"314\" src=\"https://www.youtube.com/embed/5_9mEK_4nPg\" allowfullscreen=\"allowfullscreen\" ></iframe></figure>\n<p> </p>\n<h2 id=\"software_suites\" class=\"sectionedit5\">Software suites</h2>\n<h3 id=\"mcetoc_1dhl8nk3v3\" class=\"sectionedit5\">Static Application Security Testing</h3>\n<div class=\"level3\">\n<ol>\n<li class=\"level1 node\">\n<div class=\"li\"><em class=\"u\">Multilanguage</em></div>\n<ol>\n<li class=\"level2 node\">\n<div class=\"li\">OWASP</div>\n<ol>\n<li class=\"level3\">\n<div class=\"li\">SonarQube</div>\n</li>\n<li class=\"level3\">\n<div class=\"li\">FindBugs (Java, Groovy, Scala)</div>\n</li>\n<li class=\"level3\">\n<div class=\"li\">FindSecurityBugs (Java, Groovy, Scala)</div>\n</li>\n<li class=\"level3\">\n<div class=\"li\">LAPSE+ (J2EE)</div>\n</li>\n</ol>\n</li>\n<li class=\"level2\">\n<div class=\"li\">Veracode</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">Sparrow</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">Kiuwan</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">DefenseCode Thunderscan</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">MicroFocus Fortify</div>\n</li>\n</ol>\n</li>\n<li class=\"level1 node\">\n<div class=\"li\"><em class=\"u\">Language-specific</em>:</div>\n<ol>\n<li class=\"level2 node\">\n<div class=\"li\">Python</div>\n<ol>\n<li class=\"level3\">\n<div class=\"li\">Pylint</div>\n</li>\n</ol>\n</li>\n<li class=\"level2 node\">\n<div class=\"li\">Java, JS and derivatives</div>\n<ol>\n<li class=\"level3\">\n<div class=\"li\">JLint</div>\n</li>\n<li class=\"level3\">\n<div class=\"li\">Reshift</div>\n</li>\n</ol>\n</li>\n<li class=\"level2 node\">\n<div class=\"li\">PHP</div>\n<ol>\n<li class=\"level3\">\n<div class=\"li\">Pixy</div>\n</li>\n<li class=\"level3\">\n<div class=\"li\">PHP-Sat</div>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n</div>\n<h3 id=\"dynamic_application_security_testing\" class=\"sectionedit7\">Dynamic Application Security Testing</h3>\n<div class=\"level3\">\n<ol>\n<li class=\"level1 node\">\n<div class=\"li\"><em class=\"u\">Multilanguage</em>:</div>\n<ol>\n<li class=\"level2\">\n<div class=\"li\">IBM Rational AppScan</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">Sparrow</div>\n</li>\n</ol>\n</li>\n<li class=\"level1 node\">\n<div class=\"li\"><em class=\"u\">Language-specific</em>:</div>\n<ol>\n<li class=\"level2 node\">\n<div class=\"li\">Javascript:</div>\n<ol>\n<li class=\"level3\">\n<div class=\"li\">IROH.JS</div>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n</div>\n<h3 id=\"interactive_application_security_testing\" class=\"sectionedit8\">Interactive Application Security Testing</h3>\n<div class=\"level3\">\n<ol>\n<li class=\"level1\">\n<div class=\"li\">OWASP</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Sparrow</div>\n</li>\n</ol>\n</div>\n<h3 id=\"run-time_application_security_protection\" class=\"sectionedit9\">Run-time Application Security Protection</h3>\n<div class=\"level3\">\n<ol>\n<li class=\"level1\">\n<div class=\"li\">Contrast Security</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Sparrow</div>\n</li>\n</ol>\n</div>\n</div>\n</div>\n</div>",
            "image": "https://0x30.io/media/posts/7/Dynamic-Application-Security-Test-Orchestration-1000x500.jpg",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2019-08-07T07:27:41+02:00",
            "date_modified": "2019-08-11T23:18:16+02:00"
        },
        {
            "id": "https://0x30.io/cybersecurity-kill-chain/",
            "url": "https://0x30.io/cybersecurity-kill-chain/",
            "title": "CyberSecurity &#x27;Kill Chain&#x27;",
            "summary": "What is the Cyber Kill Chain Model?‘Kill Chain’ is a term originally used by the&hellip;",
            "content_html": "<h3 id=\"mcetoc_1dhl89qlr0\">What is the Cyber Kill Chain Model?</h3>\n<p>‘Kill Chain’ is a term originally used by the military to define the steps an enemy uses to attack a target. In <a href=\"https://www.lockheedmartin.com/content/dam/lockheed/data/corporate/documents/LM-White-Paper-Intel-Driven-Defense.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">2011, Lockheed Martin released a paper</a> defining a Cyber Kill Chain. Similar in concept to the military’s model, it defines <strong>the steps used by cyber attackers in today’s cyber-based attacks</strong>. </p>\n<p>The theory is that by understanding each of these stages, defenders can better identify and stop attackers at each of the respective stages. The more points at which you can intercept the bad guys, the better the chance you have to deny them from their objective or force them to make enough noise where you can more easily detect them. </p>\n<p>The 'Kill Chain' model was developed by Lockheed Martin, named officially as the Cyber Kill Chain® framework, is part of the Intelligence Driven Defense® model for identification and prevention of cyber intrusions activity. The model identifies what the adversaries must complete in order to achieve their objective.</p>\n<p>The seven steps of the Cyber Kill Chain® enhance visibility into an attack and enrich an analyst’s understanding of an adversary’s tactics, techniques and procedures.</p>\n<p><code><strong>A: Advanced</strong></code><br><code><em>Targeted, Coordinated, Purposeful</em></code></p>\n<p><code><strong>P: Persistent</strong></code><br><code><em>Month after Month, Year after Year</em></code></p>\n<p><code><strong>T: Threat</strong></code><br><code><em>Person(s) with Intent, Opportunity, and Capability</em></code></p>\n<p><img src=\"https://notes.0x30.io/lib/exe/fetch.php?w=600&amp;tok=609fa9&amp;media=cybersec:the-cyber-kill-chain.png\"></p>",
            "image": "https://0x30.io/media/posts/6/Captura-de-pantalla-2019-08-11-a-las-23.21.44.png",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2019-08-07T07:23:56+02:00",
            "date_modified": "2019-08-11T23:22:23+02:00"
        },
        {
            "id": "https://0x30.io/databases-per-use-type/",
            "url": "https://0x30.io/databases-per-use-type/",
            "title": "Databases per use type",
            "summary": "The critical difference between NoSQL and Relational databases is that RDBMS schemas rigidly define how&hellip;",
            "content_html": "<div class=\"level1\">\n<p>The critical difference between NoSQL and Relational databases is that RDBMS schemas rigidly define how all data inserted into the database must be typed and composed, whereas NoSQL databases can be schema agnostic, allowing unstructured and semi-structured data to be stored and manipulated.</p>\n</div>\n<h2 id=\"nosql\" class=\"sectionedit2\">NoSQL</h2>\n<div class=\"level2\">\n<p>NoSQL databases emerged as a popular alternative to relational databases as web applications became increasingly complex.</p>\n<p><strong>Advantages</strong>: Since there are so many types and varied applications of NoSQL databases, it’s hard to nail these down, but generally:</p>\n<ul>\n<li class=\"level1\">\n<div class=\"li\">Schema-free data models are more flexible and easier to administer.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">NoSQL databases are generally more horizontally scalable and fault-tolerant.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">Data can easily be distributed across different nodes. To improve availability and/or partition tolerance, you can choose that data on some nodes be “eventually consistent”.</div>\n</li>\n</ul>\n<p><strong>Disadvantages</strong>: These are also dependent on the database type. Principally:</p>\n<ul>\n<li class=\"level1\">\n<div class=\"li\">NoSQL databases are generally less widely adopted and mature than RDBMS solutions, so specific expertise is often required.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">There are a range of formats and constraints specific to each database type.</div>\n</li>\n</ul>\n<p>NoSQL/Non-relational databases can take a variety of form:</p>\n</div>\n<h3 id=\"time-series_database\" class=\"sectionedit3\">Time-Series database</h3>\n<div class=\"level3\">\n<p><em>InfluxDB, AWS Timestream, Informix, Prometheus, Riak-TS, RRDTool, M3db, eXtreme</em></p>\n<p>A software implementation that is optimized for handling time series data, arrays of numbers indexed by time, a datetime or a datetime range. In some fields these time series are called profiles, curves, or traces. This kind of databases allows users to create, enumerate, update and destroy various time series and organize them. The server often supports a number of basic calculations that work on a series as a whole, such as multiplying, adding, or otherwise combining various time series into a new time serie. They can also filter on arbitrary patterns such as time ranges, low value filters, high value filters, or even have the values of one series filter another</p>\n</div>\n<h3 id=\"key-value_stores\" class=\"sectionedit4\">Key-Value Stores</h3>\n<div class=\"level3\">\n<p><em>Redis, AWS DynamoDB, Memcached, Aerospike, Riak-KV, Tarantool</em></p>\n<p>Are extremely simple database management systems that store only key-value pairs and provide basic functionality for retrieving the value associated with a known key. The simplicity of key-value stores makes these database management systems particularly well-suited to embedded databases, where the stored data is not particularly complex and speed is of paramount importance.</p>\n</div>\n<h3 id=\"wide_column_stores\" class=\"sectionedit5\">Wide Column Stores</h3>\n<div class=\"level3\">\n<p><em>Cassandra, Scylla, HBase</em></p>\n<p>Are schema-agnostic systems that enable users to store data in column families or tables, a single row of which can be thought of as a record, a multi-dimensional key-value store. These solutions are designed with the goal of scaling well enough to manage petabytes of data across as many as thousands of commodity servers in a massive, distributed system.</p>\n</div>\n<h3 id=\"document_stores\" class=\"sectionedit6\">Document Stores</h3>\n<div class=\"level3\">\n<p><em>MongoDB, Couchbase, CouchDB, Firebase,</em></p>\n<p>Are schema-free systems that store data in the form of JSON documents. Document stores are similar to key-value or wide column stores, but the document name is the key and the contents of the document, whatever they are, are the value. In a document store, individual records do not require a uniform structure, can contain many different value types, and can be nested. This flexibility makes them particularly well-suited to manage semi-structured data across distributed systems.</p>\n</div>\n<h3 id=\"graph_databases\" class=\"sectionedit7\">Graph Databases</h3>\n<div class=\"level3\">\n<p><em>Neo4J, Datastax</em></p>\n<p>Represent data as a network of related nodes or objects in order to facilitate data visualizations and graph analytics. A node or object in a graph database contains free-form data that is connected by relationships and grouped according to labels. Graph-Oriented Database Management Systems (DBMS) software is designed with an emphasis on illustrating connections between data points. As a result, graph databases are typically used when analysis of the relationships between heterogeneous data points is the end goal of the system, such as in fraud prevention, advanced enterprise operations, or Facebook’s original friends graph.</p>\n</div>\n<h2 id=\"sql_rdbms_relational\" class=\"sectionedit8\">SQL / RDBMS / Relational</h2>\n<div class=\"level2\">\n<p><em>MySQL, MariaDB, PostgreSQL, SQLite, Oracle, MS SQL Server, IBM DB2</em></p>\n<p>Relational databases emerged in the 70’s to store data according to a schema that allows data to be displayed as tables with rows and columns. RDBMS all provide functionality for reading, creating, updating, and deleting data, typically by means of Structured Query Language (SQL) statements. The tables in a relational database have keys associated with them, which are used to identify specific columns or rows of a table and facilitate faster access to a particular table, row, or column of interest.</p>\n<p>Data integrity is of particular concern in relational databases, and RDBMS use a number of constraints to ensure that the data contained in the tables is reliable and accurate.</p>\n<p><strong>Advantages</strong>:</p>\n<ul>\n<li class=\"level1\">\n<div class=\"li\">Relational databases are well-documented and mature technologies, and RDBMS are sold and maintained by a number of established corporations.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">SQL standards are well-defined and commonly accepted.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">A large pool of qualified developers have experience with SQL and RDBMS.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">All RDBMS are ACID-compliant, meaning they satisfy the requirements of Atomicity, Consistency, Isolation, and Durability.</div>\n</li>\n</ul>\n<p><strong>Disadvantages</strong>:</p>\n<ul>\n<li class=\"level1\">\n<div class=\"li\">RDBMS don’t work well (or at all) with unstructured or semi-structured data, due to schema and type constraints. This makes them ill-suited for large analytics or IoT event loads.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">The tables in your relational database will not necessarily map one-to-one with an object or class representing the same data.</div>\n</li>\n<li class=\"level2\">\n<div class=\"li\">When migrating one RDBMS to another, schemas and types must generally be identical between source and destination tables for migration to work (schema constraint). For many of the same reasons, extremely complex datasets or those containing variable-length records are generally difficult to handle with an RDBMS schema.</div>\n</li>\n</ul>\n</div>",
            "image": "https://0x30.io/media/posts/9/database-wordcloud.jpg",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2019-08-07T02:31:00+02:00",
            "date_modified": "2019-08-11T23:24:03+02:00"
        },
        {
            "id": "https://0x30.io/security-kpis/",
            "url": "https://0x30.io/security-kpis/",
            "title": "Security KPIs",
            "summary": "Building meaningful KPIs isn't an easy task. My experience with several Enterprise IT and security&hellip;",
            "content_html": "<div class=\"level1\">\n<p>Building meaningful KPIs isn't an easy task.</p>\n<p>My experience with several Enterprise IT and security methodologies and frameworks is that, due to the fact the <em>a)</em> organisations are mostly immature in regard to security, and <em>b)</em> organisations want to have a security framework in place they can trust because they don't understand the full risk picture, involvement with fat frameworks, highly theoretical and difficult to develop in the practice, are very common specially if you think that no enough headcount is normally assigned to the security practice. With that in mind, we can/must simplify the security controls by revising the most necessary indicators, getting rid of those others that only mature organisations, from security angle, are ready to deliver.</p>\n<p>On the other hand, the framework or methodology to follow must be the most simplistic posible in order to streamline the control task, avoiding as much as posible the burden of the bureaucracy typically implemented on immature organisations as a result of false security sensation when, in fact, they are simply less developed organisations trying to overcome an difficult IT practice to control risks.</p>\n</div>\n<h2 id=\"control_objectives\" class=\"sectionedit2\">Control objectives</h2>\n<div class=\"level2\">\n<ol>\n<li class=\"level1\">\n<div class=\"li\">Cloud strategy</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Stakeholder communication plan</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Security cartography</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Documented shared responsibility model + RACI</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Security Operations playbook + Run-books + RACI</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Security epics plan / Director plan</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">Incident response simulation</div>\n</li>\n</ol>\n</div>\n<h2 id=\"measurement\" class=\"sectionedit3\">Measurement</h2>\n<h3 id=\"typology\" class=\"sectionedit4\">Typology</h3>\n<div class=\"level3\">\n<ul>\n<li class=\"level1\">\n<div class=\"li\">[U] –&gt; Unaddressed</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">[E] –&gt; Engaged</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">[C] –&gt; Completed</div>\n</li>\n</ul>\n</div>\n<h3 id=\"values\" class=\"sectionedit5\">Values</h3>\n<div class=\"level3\">\n<ul>\n<li class=\"level1\">\n<div class=\"li\">[U] –&gt; 0 or 'Not addressed'</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">[E] –&gt; 1 or 'Addressed in architecture and plans'</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">[E] –&gt; 2 or 'Minimal viable implementation'</div>\n</li>\n<li class=\"level1\">\n<div class=\"li\">[C] –&gt; 4 or 'Enterprise-ready production implementation'</div>\n</li>\n</ul>\n</div>",
            "image": "https://0x30.io/media/posts/8/security_ciso-100593407-primary.idge.jpg",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2019-08-07T01:30:00+02:00",
            "date_modified": "2019-08-11T23:25:47+02:00"
        },
        {
            "id": "https://0x30.io/python-restful-apis-running-on-containers-the-easy-way/",
            "url": "https://0x30.io/python-restful-apis-running-on-containers-the-easy-way/",
            "title": "Python RESTful APIs running on containers, the easy way",
            "summary": "Writing a public article can be really difficult task. Trying to connect with the readers&hellip;",
            "content_html": "<p id=\"f0bc\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\">Writing a public article can be really difficult task. Trying to connect with the readers of all kinds can be tricky. Offer something for reading both to advanced and entry users on technology, bearing in mind how fast the technology ecosystems changes is definitely a challenge. But, on the other hand, if no one writes about his/her story, at the end of the day, nothing is written. So, I’ll try to describe here a personal situation I had some time ago. Can or cannot be fitted to today, but at least, this was a real use case. Let’s start from the very beginning.</p>\n<h2 id=\"0592\" class=\"gk gl dg bt bs di mi mj mk ml mm mn mo mp mq mr go\"><strong class=\"bi\">Table of Contents</strong></h2>\n<ol>\n<li><em class=\"ng\">Why Python?</em></li>\n<li><em class=\"ng\">Which tools do I use to program in Python?</em></li>\n<li><em class=\"ng\">Microservices and Python</em></li>\n<li><em class=\"ng\">What a microservice looks like in Python?</em></li>\n<li><em class=\"ng\">Structure of a Python microservice</em></li>\n<li><em class=\"ng\">An example “echo” microservice in Python</em></li>\n<li><em class=\"ng\">Testing and analyzing the code</em></li>\n<li><em class=\"ng\">Dockerfile: the containerization process</em></li>\n</ol>\n<h2 id=\"0ec3\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\">Why Python?</h2>\n<p id=\"a8f7\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >For a security practitioner (what is my case), Python represents more than a simple interpreted language. Python is a Swiss knife. It is present from code snippets, to Wireshark sniffer as dissectors to full GUI applications or front-ends.</p>\n<p id=\"072f\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >If your skills allow you to confront programming, or at least if you can review and modify code, Python it’s the screwdriver that will allow you to handle all the cloud/systems/applications/software/security machinery without a huge effort. Serve as an example the fact that AWS and GCP, among others, are using it heavily to build its cloud offering and tool chain. Even we have it in use in the house, with <a href=\"http://www.ansible.com/\" class=\"bc co nr ns nt nu\">Ansible</a>, a configuration management and orchestration engine built on top of Python.</p>\n<p id=\"1d4e\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >With the current market trends, that imposes an API-first directive, and the free of choice in the selection of the programming language, Python is probably one of the most flexible tools for an enterprise architect. It’s powerful, it’s relatively simple to learn (the learning curve is not so hard), it’s really well documented, can be used naked, through micro-frameworks or with fat-frameworks, and surely the most important detail, has a magnificent community around.</p>\n<p id=\"fb64\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Personally, and this is a matter of likes and dislikes, I prefer to use Python the most naked I can, but, without losing my time in rather generic and very low-level actions. For that reason, and in direct dependency of the type of software piece that needs to be written, my first option is to use Python with a <strong class=\"mu nv\">micro-framework</strong>. Among the available ones, my preference is <a href=\"http://flask.pocoo.org/\" class=\"bc co nr ns nt nu\"><strong class=\"mu nv\">Flask</strong></a>. Flask allows me to program in Python very low level, as I want, but, at the same time, it allows me to obviate all the hard work needed for generic things, like in example, handle an HTTP or TCP connection. Connection is handled by the micro-framework, but, If I want to interact with the session yet I can do it. That’s fantastic. It allows me get concentrated on programming the business logic and I only need to mess with the very low-level actions if I really want or need.</p>\n<h2 id=\"a7c2\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\">WHICH TOOLS DO I USE TO PROGRAM IN PYTHON?</h2>\n<p id=\"3fd0\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >Probably, one of the most asked question: Which tools do you use when you do program? There’s no simple, single, valid answer here. Again, this is a matter or likes and dislikes, but, as I need to present the tools you later are going to see in this article’s screenshots, this is the list of tools that you will see:</p>\n<p id=\"1f83\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >· <a href=\"https://www.python.org/downloads/\" class=\"bc co nr ns nt nu\">Python interpreter</a>, currently in version 3.7<br>· <a href=\"https://www.jetbrains.com/pycharm/download\" class=\"bc co nr ns nt nu\">PyCharm</a> Community IDE.<br>· <a href=\"https://www.getpostman.com/\" class=\"bc co nr ns nt nu\">Postman</a> API Development Environment.<br>· <a href=\"https://git-fork.com/\" class=\"bc co nr ns nt nu\">Fork</a> git client or Atlassian’s <a href=\"https://www.sourcetreeapp.com/\" class=\"bc co nr ns nt nu\">SourceTree</a> git client.<br>· <a href=\"https://www.sequelpro.com/\" class=\"bc co nr ns nt nu\">Sequel Pro</a> (Mac) or <a href=\"https://www.webyog.com/product/sqlyog\" class=\"bc co nr ns nt nu\">SQLYog</a> Community Edition (Windows, Linux).<br>· <a href=\"https://www.mozilla.org/en-US/firefox/developer/\" class=\"bc co nr ns nt nu\">Firefox Developer Edition</a>.<br>· <a href=\"https://store.docker.com/\" class=\"bc co nr ns nt nu\">Docker Community Edition</a>, stable channel.</p>\n<p id=\"7645\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >All the above tools are conscientiously selected, as all of them (mostly) are multi-platform (Windows, Linux, Mac).</p>\n<h2 id=\"d561\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\"><strong class=\"bi\">MICROSERVICES AND PYTHON</strong></h2>\n<p id=\"6feb\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >I know that you already know what a microservice is, but, I would like to define it again following a less common way:</p>\n<p id=\"d6b8\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >· Microservices are the highest exponent of a flexible software architecture.<br>· Microservices are the empiric demonstration that software can be delivered:<br>o Being strongly modularized.<br>o Providing easy replaceability capabilities.<br>o Enabling sustainable development efforts.<br>o Simplifying legacy software refactory, rearchitecture, rebuild and replacement capabilities.<br>o Speeding time to market (this is an interesting topic that can be worth of a single article illustrating the reasons and procedures).<br>o Enabling independent scaling.<br>o Providing free of choice of technologies.<br>o Finally, all of the above, but on a <strong class=\"mu nv\">Continuous Delivery</strong> manner.<br>· Microservices are typically developed by two-pizza teams (6–8 people at most).<br>· Microservices has no more than few hundred lines of code, let’s say ~ 200 lines at most. We should not stick to this, but, if you want to use “Lines of Code” (LoC) metric in order to determine how fast can be a microservice architecture deployment, as a rough example, think for a while a good programmer typically writes a range of 10 to 30 good lines of code per day. There’s an explanation for this: depending on the language, and specially with interpreted languages, a single line of Python requires about 40% more mental load compared with a single line of C, Java, JavaScript or CSS and probably it implements at least twice as much functionality. I’ll resume it for you: <strong class=\"mu nv\">it’s blazing fast.<br></strong>· Microservices are totally decoupled among them. Databases, typically used in the software industry as a coupling mechanism, here are only an implementation detail.<br>· Microservices require a test-oriented development approach (like DDD, TDD, ATDD, BDD) to maximize its capacity of use and reduce coding errors.<br>· Microservices can be versioned at will, meanwhile a common interface is available (an API).<br>· Microservices can be written on different languages. Meanwhile they can interact each other using a common interface (an API) there’s no problem.</p>\n<h2 id=\"1e25\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\"><strong class=\"bi\">WHAT A MICROSERVICE LOOKS LIKE IN PYTHON</strong></h2>\n<p id=\"3ac4\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >With a clear dependence on the language in use, normally a microservice is exactly that, a “micro” program acting as a server or backend service. It can be as small as the following:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"e71c\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">from flask import Flask</em></span><span id=\"2be7\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">app = Flask(__name__)</em></span><span id=\"f1bb\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">@app.route(‘/’)<br>def index():<br>    return “Hello, World!”</em></span><span id=\"605f\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">app.run(debug=True)</em></span></pre>\n<p id=\"8fa5\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Let’s examine the above code:</p>\n<p id=\"f005\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><em class=\"ng\">from flask import Flask</em></p>\n<p id=\"2cbf\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This is a python import directive. Flask is a micro-framework, thus is, a set of python files written by other that we want to import into our main programme file. It’s necessary to use the Flask micro framework.</p>\n<p id=\"05c1\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><em class=\"ng\">app = Flask(__name__)</em></p>\n<p id=\"b987\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Here we declare that our app is going to be a Flask app. The special operand __name__ is equal to the name of the file where we are writing the code, in our case, app.py</p>\n<p id=\"ce11\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><em class=\"ng\">@app.route(‘/’)</em></p>\n<p id=\"721b\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This is a decorator. Decorators are professional programmer’s technique to remove unwanted code from screen, hiding pieces of code in libraries. In this case, this decorator belongs to the Flask micro-framework libraries and it allows to set a portion of the URL available for a function to execute its code.</p>\n<p id=\"211c\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><em class=\"ng\">def index():</em></p>\n<p id=\"be1a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This statement defines a function. This function is going to be executed every time the URL route defined before by a Flask decorator is called via URL.</p>\n<p id=\"2176\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><em class=\"ng\">return “Hello, World!”</em></p>\n<p id=\"10ab\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This is what is going to be sent back to the client when the route decorator path is called on a URL.</p>\n<p id=\"3e59\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><em class=\"ng\">app.run(debug=True)</em></p>\n<p id=\"2743\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This is the statement that effectively runs the Python program defined before. And it will be executed with debugging details on <em class=\"ng\">stdout</em> and <em class=\"ng\">stderr</em>.</p>\n<h2 id=\"c02b\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\"><strong class=\"bi\">STRUCTURE OF A PYTHON MICROSERVICE</strong></h2>\n<p id=\"ab8f\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >As a contrast to traditional monoliths, microservices have a very simplistic approach. Most of them are programs contained on a single file, with a short list of calls to another programs or libraries (what is called “includes”) and with a rather simple or even without folder structure.</p>\n<p id=\"f66d\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The example program we are going to build is kept for learning purposes in a repository on the public GitHub. The project is this:</p>\n<p id=\"573e\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><a href=\"https://github.com/ea1het/PythonAPIContainer\" class=\"bc co nr ns nt nu\"><strong class=\"mu nv\">https://github.com/ea1het/PythonAPIContainer</strong></a></p>\n<p id=\"7a86\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >You can clone it using one of the following two command lines:</p>\n<p id=\"69ed\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >a) git clone <a href=\"https://github.com/ea1het/PythonAPIContainer.git\" class=\"bc co nr ns nt nu\">https://github.com/ea1het/PythonAPIContainer.git</a><br>b) git clone <a href=\"mailto:git@github.com:ea1het/PythonAPIContainer.git\" class=\"bc co nr ns nt nu\">git@github.com:ea1het/PythonAPIContainer.git</a></p>\n<p id=\"8667\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >When you are programming larger applications on fat frameworks like Django, the folder structure, and the application configuration file, are key elements that will make your program behave smooth, or otherwise you would end up having a mess in the code. In that sense, and despite a microservice needs to be small in essence, I yet think a good folder structure, if needed, and a configuration file is a very convenient feature. Let’s see how a microservice is organized and how a configuration file looks like</p>\n<p id=\"75b7\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">a)</strong> <strong class=\"mu nv\">Folder / File structure</strong></p>\n<p id=\"c25e\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Due to the fact a microservice is small enough, in almost all cases the folder structure won’t exist. All the potential files in use will be in flat on a single folder or directory. This can be a problem or a virtue, depending on the place where you intend to run code. In example, running a microservice in AWS as a lambda function imposes the obligation to have the file with the main programme in the application’s folder root. In contrast, a very typical in Python, is the use of application factories. An application factory is a useful pattern where you create your application using a function, an object. This allows you to pass different configurations to the main program, for example for unit testing, and also allows you to run multiple instances of the same application running in the same application process. That is a nice trick senior programmers tend to use to avoid circular dependency includes and to reuse code and server resources, albeit can be difficult to understand for novice programmers. Anyway, the important point here is the fact lambda functions and application factories are almost incompatible. That will explain further decisions taken on design.</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"4f04\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">./<br>├── .env<br>├── Dockerfile<br>├── __init__.py<br>├── app.py<br>├── boot.sh<br>├── requirements.txt<br>├── settings.py<br>└── venv</em></span></pre>\n<p id=\"3696\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The above contents aren’t compulsory “as-is”. They just reflect the way I’m creating a microservice. Let me explain what’s the usage of each file:</p>\n<ul>\n<li id=\"763a\"><strong class=\"mu nv\">.env</strong>: This is a very special file. It holds the configuration variables your program requires in order to start working. This file is special because only changing this file’s contents you can change the way your program behaves. If you configure this file properly you can use your program on QA stage and with minor adjustments only on this file your program can be ready to be deployed in production. The name represents the type of configuration strategy followed, that is environment variables. Environmental variables are one of the “12 Factor App” principles in software development. Sensible parameters should never be hardcoded in programs. Environment variable loading is a clever way to avoid hardcoding. In addition to that, the name of the file is “.env”, with a dot (“.”) at the beginning of the name. On UNIX-like systems, a file starting with a dot is a hidden file. Tools like Git, massively deployed on UNIX-like systems, won’t see a .env file (by convention) so this file won’t be staged during development, thus, avoiding the hassle to send sensible credentials to a git repository.</li>\n<li><strong class=\"mu nv\">Dockerfile</strong>: This file is the script that will convert this microservice written in Python into a container image that later on we can run on production with, i.e. Rancher Labs, our container orchestrator.<br><br></li>\n<li><strong class=\"mu nv\">__init__.py</strong>: This is a very important file in Python language. The __init__.py files are required to make Python treat the directories as containing packages; this is done to prevent directories with a common name, such as “string”, from unintentionally hiding valid modules that occur later, deeper, on the module search path. In the simplest case, __init__.py can just be an empty file or even it’s not needed (our case can be a good candidate to remove this file), but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc… It can also execute initialization code for the package or set variables. If you remove the __init__.py file, Python will no longer look for submodules inside that directory, so, attempts to import one of those submodules will fail.<br><br></li>\n<li><strong class=\"mu nv\">app.py</strong>: This is the microservice itself. Here is where the business logic is coded.<br><br></li>\n<li><strong class=\"mu nv\">boot.sh</strong>: This file holds the initialization script the microservice requires when it has been converted into a container. In few words, this is the script that will be executed inside the container image once the container orchestrator, or Docker, call it into execution.<br><br></li>\n<li><strong class=\"mu nv\">requirements.txt</strong>: This file holds the list of dependencies your code has generated. Basically, it’s the list of additional packages your code requires in order to work as expected.<br><br></li>\n<li><strong class=\"mu nv\">settings.py</strong>: This file is a configurator file. It holds different execution types your program requires for, i.e. development stage, QA stage or production stage.<br><br></li>\n<li><strong class=\"mu nv\">venv/</strong>: This is a folder with Python’s virtual environment, where Flask and other dependencies are installed.</li>\n</ul>\n<p id=\"62e7\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">a)</strong> <strong class=\"mu nv\">Microservice configuration file in Python</strong></p>\n<p id=\"1ca8\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >As explained before, configuration files and application factories are quite typical practice for senior programmers. Unfortunately, in the scope of a microservice implementation I don’t consider essential an application factory given the fact that each microservice is going to run independently, in the form of a lambda function or container, that is going to be executed several times on an auto-scaled orchestration or choreography. For that reason, application factory pattern “per se” is not in use, albeit a configuration file with that appearance is in use.</p>\n<p id=\"b2e3\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >A good configuration file follows the “12 Factor App” methodology and principles, as long as the microservice itself should do. If it’s done the right way, it will enable applications to be built with portability and resiliency when deployed to the Web. Other benefits of the “12 Factor App” are:</p>\n<ul>\n<li id=\"0c1a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Uses declarative formats for setup automation, to minimize time and cost for new developers joining the project.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Have a clean contract with the underlying operating system, offering <strong class=\"mu nv\">maximum portability</strong> between execution environments.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Are <strong class=\"mu nv\">suitable for deployment on modern cloud platforms</strong>, obviating the need for servers and systems administration.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Minimize divergence or drift between development and production, enabling <strong class=\"mu nv\">continuous deployment</strong> for <strong class=\"mu nv\">maximum agility</strong>.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">And can scale up without significant changes to tooling, architecture, or development practices.</li>\n</ul>\n<p id=\"9719\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Specifically, on configuration file matters, Factor III of the “12 Factor App” methodology and principles states that […] <em class=\"ng\">configuration that varies between deployments should be stored in the environment.</em> […], so, this is what we are going to do in our configuration file, given the fact that from an architecture standpoint, <strong class=\"mu nv\">every piece of code will always traverse, at least, 3 different stages: 1/development, 2/test and 3/production.</strong></p>\n<p id=\"ceb4\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >At this point, a clarification is needed: an application’s configuration is everything that is likely to vary between named deploys. This includes, among possibly others:</p>\n<ul>\n<li id=\"78b7\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Resource handles to the database, caches, and other backing services.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Credentials to external services such as Amazon S3 or AAA mechanisms.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Per-deploy values such as the canonical hostname for the deploy.</li>\n</ul>\n<p id=\"20e6\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Applications sometimes store configuration as constants in the code. This is what is known as “hardcoding”. This is a violation of basic security measures applied in development. Accessing the code, in development, test or production, means compromising any credentials used, as well as the systems related. On the other hand, note that the definition of “configuration” does not include internal application configuration, or how coded modules are connected among them. This type of configuration does not vary between deploys, therefore, this is not what we want to control.</p>\n<p id=\"b7ce\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The “<a href=\"https://12factor.net/\" class=\"bc co nr ns nt nu\">12 Factor App</a>” methodology and principles propose storing configuration in <strong class=\"mu nv\">environment variables (often shortened to env vars or env)</strong>. Env vars are easy to change between deploys without changing any code. Unlike configuration files, there is little chance of them being checked into the Git code repository accidentally (as a trick, normally they are hidden files that passes unnoticed to Git), and, unlike custom configuration files, or other configuration mechanisms such as Java System Properties, they are a language-agnostic and OS-agnostic standard. Env vars are granular controls, each fully orthogonal to other env vars. Ideally, they are never grouped together as “environments”. This is a model that scales up smoothly as the application can expands into more deploys over its lifetime. Albeit the general recommendation is good, working with thousands of developers at the same time imposes the obligation to standardize environments used in the DevSecOps pipeline. Therefore, we’ll treat the last recommendation as such, and in regard to env vars and grouped environments will be rather a “relaxed” that recommendation.</p>\n<p id=\"5181\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Finally, after all this long description, this is what a microservice configuration file looks like in Python. Pay special attention to the line in colour red:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"5c97\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">””” Microservice configurator file ”””<br>##<br>#<br># This file configures the microservice.<br>#<br># This microservice uses ‘dotenv’ (.env) files to get environmental<br># variables. #This is done to configure sensible parameters, like<br># database connections,application secret keys and the like. In case <br># the ‘dotenv’ file does not<br># exists, a warning is generated at run-time.<br>#<br># This application implements the ’12 Factor App’ principle:<br># </em><a href=\"https://12factor.net/\" class=\"bc co nr ns nt nu\"><em class=\"ng\">https://12factor.net</em></a><em class=\"ng\"> and </em><a href=\"https://12factor.net/config\" class=\"bc co nr ns nt nu\"><em class=\"ng\">https://12factor.net/config</em></a><em class=\"ng\"><br>#<br># Note about PyLint static code analyzer: items disabled are false <br># positives.<br>#<br>##</em></span><span id=\"96df\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># pylint: disable=too-few-public-methods;<br># In order to avoid false positives with Flask</em></span><span id=\"bf94\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">from os import environ, path<br>from environs import Env</em></span><span id=\"bdfe\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">ENV_FILE = path.join(path.abspath(path.dirname(__file__)), ‘.env’)</em></span><span id=\"fe7b\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">if path.exists(ENV_FILE):<br>    ENVIR = Env()<br>    ENVIR.read_env()<br>else:<br>    print(‘Error: .env file not found’)<br>    exit(code=1)</em></span><span id=\"3431\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">class Config:<br>””” This is the generic loader that sets common attributes ”””<br>JSON_SORT_KEYS = False<br>DEBUG = TRUE<br>TESTING = TRUE</em></span><span id=\"2419\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">class Development(Config):<br>””” Development loader ”””<br>ENV = ‘development’<br>if environ.get(‘KEY_DEVL’):<br>    SECRET_KEY = ENVIR(‘KEY_DEVL’)<br>if environ.get(‘DATABASE_URI_DEVL’):<br>    DATABASE_URI = ENVIR(‘DATABASE_URI_DEVL’)<br>TESTING = False</em></span><span id=\"c283\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">class Testing(Config):<br>””” Testing loader ”””<br>ENV = ‘testing’<br>if environ.get(‘KEY_TEST’):<br>    SECRET_KEY = ENVIR(‘KEY_TEST’)<br>if environ.get(‘DATABASE_URI_TEST’):<br>    DATABASE_URI = ENVIR(‘DATABASE_URI_TEST’)<br>DEBUG = False</em></span><span id=\"b976\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">class Production(Config):<br>””” Production loader ”””<br>ENV = ‘production’<br>if environ.get(‘KEY_PROD’):<br>    SECRET_KEY = ENVIR(‘KEY_PROD’)<br>if environ.get(‘DATABASE_URI_PROD’):<br>    DATABASE_URI = ENVIR(‘DATABASE_URI_PROD’)<br>DEBUG = False<br>TESTING = False</em></span></pre>\n<p id=\"7742\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >If you did realize the above marked coloured in red, this is a call to a hidden file named “.env” (“dot” “env”). <strong class=\"mu nv\">A “dot” “env” file is a file holding “env vars”. Get the trick? Remember I said a file starting with a “dot” is a hidden file, a file that Git does not recognize? Hey! You got it know!</strong></p>\n<p id=\"22fe\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This “dot” “env” file, and not the “configuration file” listed before, is the real configuration file holding environmental variables as the “12 Factor App” defines. That’s the reason because before I said the […]<em class=\"ng\"> recommendation on env vars and grouped environments will be “relaxed” in most cases.</em></p>\n<p id=\"00aa\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The content of this “dot” “env” file holding env vars is, for example, like this:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"0475\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">#<br>#<br># This is the configuration file for this microservice<br>#<br># =================================================================<br>#<br># MODE_COMFIG variable will affect the way the application is run.<br># Available running modes are:<br>#<br># — Development<br># — Testing<br># — Production<br>#<br>export MODE_CONFIG = ‘Production’<br>export KEY_PROD = ‘example–0000–7890-aaaa-54a063e6d5a3’<br>export KEY_DEVL = ‘example-1111–7890-bbbb-5ddc2947910c’<br>export KEY_TEST = ‘example-2222–7890-cccc-0da6c41293a8’<br>export DATABASE_URI_PROD = ‘mysql+pymysql://user:pass@127.0.0.1:3306/database’<br>export DATABASE_URI_DEVL = ‘mysql+pymysql://user:pass@127.0.0.1:3306/database’<br>export DATABASE_URI_TEST = ‘sqlite:///test_app.db’<br>export REDIS_CACHE_URI = ‘’<br>export AWS_ACCESS_KEY = ‘’<br>export AWS_SECRET_KEY = ‘’</em></span></pre>\n<p id=\"30b7\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Before moving into another related topics, I want to cycle back for a while on security aspects, the “12 Factor App” and the env vars.</p>\n<p id=\"7381\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">a)</strong> <strong class=\"mu nv\">Is secure to use env vars?</strong></p>\n<p id=\"86cd\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Saying sharply “yes” or “not”, without some supportive arguments, for me, could be erroneous. Let me elaborate a bit on this matter but, also let me start my argumentation with a clear statement: <strong class=\"mu nv\">process environments are only accessible to the user that owns the process, and root, of course</strong>.</p>\n<p id=\"d9e4\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">Environment variables are plenty secure</strong>. The question in itself is loosely related to the real concern behind using env vars, that is, <em class=\"ng\">What happens if the system is compromised?</em> Well, in such case, with the system already compromised, the less of the problems is the use of env vars. In fact, that’s funny because even in such scenario, there’s a silly security benefit of using env vars over configuration files and, amazingly, it is obscurity. Meaning that, if someone has gained root access, (s)he can get to both, files and env vars, but env vars will be less apparent at glance.</p>\n<p id=\"0d32\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >On a modern UNIX-like system, you can only access the data stored in an environment variable in 5 different situations, all of them with native controls:</p>\n<p id=\"147a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">1. The running environment of the process:</strong></p>\n<p id=\"1bc1\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >When the process is running, the environment variables of that process can be accessed through /proc/$PID/environ. However, only the user who owns the process, or root, can access that file.</p>\n<p id=\"26d1\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">2. The source of the environment variables:</strong></p>\n<p id=\"1fc1\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >If you’re using an init script, and the variables are stored in that init script, the variables can of course be obtained by reading that script, or, if the env vars are coming from somewhere else, then wherever that is. Permissions on files here are the key factor to consider.</p>\n<p id=\"1936\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">3. Executing the <em class=\"ng\">‘ps’</em> command and reviewing the output:</strong></p>\n<p id=\"1918\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >If the process is (badly) launched via something like…</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"9630\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">sh -c ‘cd /foo/bar; POP=star /my/executable’</em></span></pre>\n<p id=\"3085\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >…then that “<em class=\"ng\">sh”</em> process will be visible in the “<em class=\"ng\">ps”</em> command output. The portion of the above code in red is an env var assignment (badly) done in run-time.</p>\n<p id=\"3126\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">4. During the “execve” system call:</strong></p>\n<p id=\"e0f4\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The “execve” system call does not directly leaks the environment. However, there is a generic audit mechanism that can log the arguments of every system call, including execve. So, if the UNIX system auditing is enabled, the environment can be sent through the audit mechanism and end up in a log file. On a decently configured system, only the root has this access, that means, data is sent to, i.e., to /var/log/audit/audit.log, that is only readable by root, and written by the auditd daemon, that is also running as root.</p>\n<p id=\"142a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">5. During a “ptrace” system call:</strong></p>\n<p id=\"8e98\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This system call allows a process to inspect the memory and even execute code in the context of another process. It’s what allows debuggers to exist. Only a specific UID can trace its own processes. Furthermore, if a process is privileged (setuid or setgid), only root can trace it.</p>\n<p id=\"c373\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >So, as a succinct resume, one unprivileged user can’t observe env vars for another user through the process table (<em class=\"ng\">“ps auxwwe” </em>command). If somebody is observing another user’s process, that is root. The commands that set environment variables (i.e., <em class=\"ng\">export</em>) are shell builtins which don’t make it onto the process table and, by extension, aren’t in /proc/$pid/cmdline. Finally, /proc/$pid/environ is only readable by the UID of the process owner.</p>\n<p id=\"a23d\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Env vars are an essential part of an UNIX-like system. That doesn’t mean they must be used for everything, or they are best suited that any other technique. That wouldn’t be true. The problem with env vars is allowing anyone to modify environment variables of significance. And that doesn’t happen by weird hacking techniques, no, that happens by wrong programming actions. Programmers should always be cautious as to what data their programs accept and use for subsequent processing/directives.</p>\n<h2 id=\"7244\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\"><strong class=\"bi\">AN EXAMPLE “ECHO” MICROSERVICE IN PYTHON</strong></h2>\n<p id=\"ff36\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >In order to demonstrate how a microservice can be quickly crafted in Python, we’re going to build an “echo” program. An echo program will be a simple microservice containing two namespaces:</p>\n<ul>\n<li id=\"cfce\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">One namespace acting as URL index, the well-known ‘/’.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">One namespace accepting POSTing in the “/echo” and /echo/&lt;value&gt; URLs.</li>\n</ul>\n<p id=\"0c76\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The example code follows:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"f236\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">“”” Microservice main programm file “””<br>##<br>#<br># This file is the microservice itself.<br>#<br>##</em></span><span id=\"8a30\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># pylint: disable=invalid-name; <br># In order to avoid false positives with Flask</em></span><span id=\"333c\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">from os import environ<br>from datetime import datetime<br>from flask import Flask, jsonify, make_response, url_for, request<br>import settings</em></span><span id=\"e1e9\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># — Application initialization. — — — — — — — — — — — — — — — — — — — <br>__modeConfig__ = environ.get(‘MODE_CONFIG’) or ‘Development’<br>APP = Flask(__name__)<br>APP.config.from_object(getattr(settings, __modeConfig__.title()))</em></span><span id=\"7991\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># — This functions control how to respond to common errors. — — —<br>@APP.errorhandler(404)<br>def not_found(error):<br>    “”” HTTP Error 404 Not Found “””<br>    headers = {}<br>    return make_response(<br>    jsonify({‘error’: ‘true’,‘msg’: str(error)}), 404, headers)</em></span><span id=\"bceb\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">@APP.errorhandler(405)<br>def not_allowed(error):<br>    “”” HTTP Error 405 Not Allowed “””<br>    headers = {}<br>    return make_response(jsonify({‘error’: ‘true’,‘msg’: str(error)}), 405, headers)</em></span><span id=\"89e4\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">@APP.errorhandler(500)<br>def internal_error(error):<br>    “”” HTTP Error 500 Internal Server Error “””<br>    headers = {}<br>    return make_response(jsonify({‘error’: ‘true’,‘msg’: str(error)}), 500, headers)</em></span><span id=\"45cb\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># — This piece of code controls what happens during the whole <br># HTTP transaction.<br>@APP.before_request<br>def before_request():<br>    “”” This function handles HTTP requests arriving the API “””<br>    pass</em></span><span id=\"eda3\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">@APP.after_request<br>def after_request(response):<br>    “”” This function handles HTTP responses to client “””<br>    return response</em></span><span id=\"6b85\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># — This is where the API effectively starts. — — — — — — — — — — —<br>@APP.route(‘/’, methods=[‘GET’])<br>def index():<br>    “””<br>    This is the API index endpoint with HATEOAS support<br>    :param: none<br>    :return: a JSON (application/json)<br>    “””<br>    headers = {}<br>    return make_response(jsonify({‘msg’: ‘this is index endpoint’,‘tstamp’: datetime.utcnow().timestamp(),‘endpoints’: {‘url_echo’: url_for(‘echo’, _external=True)}}), 200, headers)</em></span><span id=\"17f5\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">@APP.route(‘/echo’, methods=[‘POST’])<br>@APP.route(‘/echo/&lt;string:item&gt;’, methods=[‘POST’])<br>def echo(**kwargs):<br>    “””<br>    This is the ECHO endpoint with HATEOAS support<br>    :param **kwargs: gets an item from the url as a string of any size and format<br>    :return: a JSON (application/json)<br>    “””<br>    if kwargs:<br>        content = kwargs[‘item’]<br>    else:<br>        content = ‘none’<br>    <br>    if request.args.get(‘lang’, type=str) is None:<br>        lang = ‘none’<br>    else:<br>        lang = request.args.get(‘lang’, type=str)<br>        headers = {}<br>        return make_response(jsonify({‘msg’: ‘this is an echo endpoint’,‘tstamp’:datetime.utcnow().timestamp(),‘namespace_params’: {‘content_received’: content,‘language’: lang},‘endpoints’: {‘url_index’: url_for(‘index’, _external=True)}}), 200, headers)</em></span><span id=\"5b32\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\"># — Finally, the application is run, more or less ;) — — — — — — —<br>if __name__ == ‘__main__’:<br>    APP.run()</em></span></pre>\n<p id=\"e247\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Now, let’s review the code to understand what happens in each block:</p>\n<p ><img src=\"https://miro.medium.com/max/750/1*FpioJvB6IT4tdwEIidzOeQ.png\" alt=\"Python imports\" width=\"375\" height=\"59\"></p>\nPython imports\n<p id=\"e452\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The above statements are the main application imports. At the end of the block it’s imported the configuration file with the env vars that we have defined on separate files.</p>\n<p ><img src=\"https://miro.medium.com/max/752/1*_KXK6YNSJxcAm62qeJaRxQ.png\" alt=\"Python application initialization from env vars\" width=\"376\" height=\"75\"></p>\nPython application initialization from env vars\n<p id=\"b252\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Later, the default configuration is effectively loaded. If it cannot be loaded, because the variable Is not defined or because the option in the variable is erroneous, then, a default value is configured. The default value for this microservice is “Development”. This option will act as a failsafe in our scenario, but, in other scenario other potential options can fit better, like Production or even not allowing the microservice execution without a proper configuration, which makes more sense.</p>\n<p ><img src=\"https://miro.medium.com/max/750/1*O-Yu4ipiW5epLU7R6BLFag.png\" alt=\"Typical HTTP errors managed in APIs\" width=\"375\" height=\"556\"></p>\nTypical HTTP errors managed in APIs<br>\n<p id=\"da72\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >This block controls the typical HTTP errors that we can find while executing the microservice program. May sound silly, but, controlling this tree errors, we are basically controlling around 90% of the situations we can face with an API in production.</p>\n<p id=\"eafc\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Pay special attention to the first clause after the docstring, the one that says “<em class=\"ng\">headers = {}</em>”. This clause allows to add more headers, if needed, to the default HTTP headers the web server is presenting to clients. You’ll see this in more detail a bit later.</p>\n<p id=\"b482\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The “<em class=\"ng\">jsonify</em>” clause generates a JSON-formatted response based on the key-values “<em class=\"ng\">error</em>” and “<em class=\"ng\">msg</em>”. The JSON is sent back to the client.</p>\n<p ><img src=\"https://miro.medium.com/max/750/1*iI0gMjqwVLQENhZeX-xwPA.png\" alt=\"Special functions controlling HTTP request/response transactions\" width=\"375\" height=\"131\"></p>\nSpecial functions controlling HTTP request/response transactions<br>\n<p id=\"274e\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The “<em class=\"ng\">before request</em>” and “<em class=\"ng\">after request</em>” are one of the reasons why Flask is so powerful. These two predefined decorators in the micro-framework. In combination with a function, it allows to modify the HTTP request sent by the client to the server (“<em class=\"ng\">before request</em>”) and the HTTP response originated by the API on-the-fly to the customer (“<em class=\"ng\">after request</em>”). Typical uses for this include:</p>\n<ul>\n<li id=\"4f0c\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Add headers for server-side purposes, like caching or proxying.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Add personalized eTag.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Open a database connection.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Initialize the logging facility.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Alter server response to adapt to client, i.e. to the User-Agent or View-Point.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Adapt HTTP responses to clients in the interim periods.</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/750/1*WLa8jGoyGSweH14dnHXmfg.png\" alt=\"An index namespace on a RESTful API\" width=\"375\" height=\"282\"></p>\nAn index namespace on a RESTful API<br>\n<p id=\"206a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Finally, we get to the first piece of the microservice business logic in itself. This piece of code configures the response a client will receive when it’s called the index of the domain using an HTTP GET verb/method. This response is a JSON containing:</p>\n<ul>\n<li id=\"1d46\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">A text literal indicating where the client is exactly on this API</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">A timestamp (server-side time)</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">A list browsable namespaces and endpoints this API has available (HATEOAS)</li>\n</ul>\n<p><img src=\"https://miro.medium.com/max/752/1*X4DrnOBiSVz7c41ZoBZIqA.png\" alt=\"Very basic handling of URL parameters received on a POST\" width=\"376\" height=\"343\"></p>\nVery basic handling of URL parameters received on a POST<br>\n<p id=\"d606\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >And this is the second piece of the microservice business logic. The URL endpoint is different. Before it was “<em class=\"ng\">/</em>” (the domain index) whereas now the endpoint is named “<em class=\"ng\">/echo</em>”. There’s an additional line that accepts any string literal after the “<em class=\"ng\">/echo</em>” in the form “<em class=\"ng\">/echo/&lt;string_literal&gt;</em>”. On both cases the HTTP verb/method in use is POST, so, we accept data sent from client reaching the API, but, not in any way.</p>\n<p id=\"cb70\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >After the routing options:</p>\n<ul>\n<li id=\"9c3a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">In (1), we control the arguments we are receiving from the client. Unauthenticated clients are always treated as non-trusted, so, anything coming from clients need to be quarantined. And this is exactly what we are doing with this statement. From the POST URI we load all the received POST from the client as a long string. That way, we disable potential errors while dealing with a SQLi or XSS, as we have converted the whole URI that can contain scriptable code on a text literal. Moreover, we can quickly limit the length of the string and filter the presence of unwanted special characters, source of the problem on SQL and XSS.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">In (2), we run along the POST URI looking for fixed parameters, like the language clause. We can be receiving a SQLi or an XSS but we won’t pay attention to those pieces of the POST URI, only to the elements we want to pick. If we identify them, they are captured and again converted into strings on new variables. The rest is garbage.</li>\n</ul>\n<p id=\"c248\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >In this occasion, the response sent back to the client after his/her post is another JSON, this time containing:</p>\n<ul>\n<li id=\"23ca\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">A text literal indicating where the client is exactly on this API</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">A timestamp (server-side time)</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">The string literals to be echoed</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">A list browsable namespaces and endpoints this API has available (HATEOAS)</li>\n</ul>\n<p id=\"b18c\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Finally, the application is run if the file containing the program is the main program file, and not a file included (called) by another program:</p>\n<p ><img src=\"https://miro.medium.com/max/748/1*gZdSlCIC15WGuniZzA6R4w.png\" alt=\"Main program execution\" width=\"374\" height=\"74\"></p>\nMain program execution<br>\n<p id=\"a733\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Most of the “echo” microservice lines of code are boilerplate, but, they were included in order to document the code and demonstrate good programming skills. Programmer skills, as well as code, can be tested, in fact should always be tested, and about testing is the next section.</p>\n<h2 id=\"42e2\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\"><strong class=\"bi\">TESTING AND ANALYSING THE CODE</strong></h2>\n<p id=\"ea40\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >I’m mentioning here the words “testing” and “analysing” because, for me, both terms have slightly different meanings:</p>\n<p id=\"fef6\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">Testing (the code)</strong> for me means an intrinsic development sub-process, that can take place before or after the business logic is created as code. This will depend on the type of programming paradigm being followed, i.e., Waterfall, Extreme, Agile, Lean, DDD, BDD, TDD, ATDD, etc…</p>\n<p id=\"aecd\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >In the DevSecOps model, the model we are following in the cloud industrialization sub-programme of the cloud programme that is in course, DevSecOps is about DEVelopment, SECurity, OPerationS <strong class=\"mu nv\">and QA, of course</strong>. The fact the letters Q &amp; A aren’t in the title doesn’t affect the fact <strong class=\"mu nv\">QA is essential to DevSecOps. QA is an enable</strong>r.</p>\n<p id=\"931d\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >In DevSecOps, QA is about trying to push tests with automation into the continuous integration systems. Tests must have zero human intervention and should generate their own test data. QA also works with operations on establishing monitoring tools and, perhaps, on continuously running chaos monkey tests in production to ensure a high level of maturity is achieved.</p>\n<p id=\"d8be\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">Analysing (the code)</strong> for me means an additional security layer on the way the code, and the tests, are built in order to ensure that code, holding the business logic and the testing process, is secure enough.</p>\n<p id=\"eac5\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Source code analysis can be either static or dynamic. In static analysis, debugging is done by examining the code without actually executing the program. This can reveal errors at an early stage in program development, often eliminating the need for multiple revisions later. After static analysis has been done, dynamic analysis is performed in an effort to uncover more subtle defects or vulnerabilities. Dynamic analysis consists more of real-time program testing.</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"c318\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><strong class=\"nx nv\"><em class=\"ng\">NOTE</em></strong><em class=\"ng\">: The most appropriate on a working environment is to perform both static and dynamic code analysis. While static code analysis is dead simple to achieve, the dynamic code analysis might require several external tooling, so, this part of the analysis won’t be documented extensively.</em></span></pre>\n<p id=\"d6ab\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >In Python, and with any micro-framework, testing stage is relatively simple. I won’t discuss here and now the fact of which development process/paradigm is best or when do tests need to be written. Of course, I’m a human and I have a preference but my preference is just an opinion. My preference is following BDD, thus, writing meaningful tests before writing the code, so, when you start coding the business logic, you get more concentrated on building what you specifically need to build.</p>\n<p id=\"ad4d\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >With all this introduction already set, I’ll start by approaching the nature of testing stage and later we’ll address the nature of the code analysis stage.</p>\n<p id=\"18ff\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">a)</strong> <strong class=\"mu nv\">Testing code with Python tools</strong></p>\n<p id=\"bccc\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Python provides all means for code testing. I’m not going to enter in very profound details, but it’s possible to perform all types of testing with a single unified or few distributed set of tools. All of the Open source tools, all of them, are detailed on <a href=\"https://wiki.python.org/moin/PythonTestingToolsTaxonomy\" class=\"bc co nr ns nt nu\">https://wiki.python.org/moin/PythonTestingToolsTaxonomy</a>. In order to selected a tool, you have to have clear what’s what you try to test:</p>\n<ul class=\"\">\n<li id=\"038a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf ot ou ov\" ><strong class=\"mu nv\">Unit test</strong>: Typically, “mock” or “dummy” implementations. Specify and test one point of the contract of single method of a class. This should have a very narrow and well-defined scope. Complex dependencies and interactions to the outside world are stubbed or mocked.</li>\n<li id=\"121c\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">Integration test</strong>: Test the correct inter-operation of multiple subsystems. There is whole spectrum there, from testing integration between two classes, to testing integration with the production environment.</li>\n<li id=\"494c\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">Smoke test (aka Sanity check)</strong>: A simple integration test where we just check that when the system under test is invoked it returns normally and does not blow up suddenly:</li>\n<li id=\"dc3f\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" >Smoke testing is both an analogy with electronics, where the first test occurs when powering up a circuit. if it smokes, of course, it’s bad :) …</li>\n<li id=\"a7f3\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" >… and, apparently, with plumbing, where a system of pipes is literally filled by smoke and then checked visually. If anything smokes, the system is leaky, which is also bad :)</li>\n<li id=\"7744\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">Regression test</strong>: A test that was written when a bug was fixed. It ensures that this specific bug will not occur again. The full name is “non-regression test”. It can also be a test made prior to changing an application to make sure the application provides the same outcome.</li>\n</ul>\n<p id=\"d8c7\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >To the above, I will add:</p>\n<ul class=\"\">\n<li id=\"e9ad\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf ot ou ov\" ><strong class=\"mu nv\">Acceptance test</strong>: Test that a feature or use case is correctly implemented. It is similar to an integration test, but with a focus on the use case to provide rather than on the components involved.</li>\n<li id=\"8853\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">System test</strong>: Tests a system as a black box. Dependencies on other systems are often mocked or stubbed during the test, otherwise, it would be more of an integration test.</li>\n<li id=\"05a7\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">Pre-flight check</strong>: Tests that are repeated in a production-like environment, to alleviate the <em class=\"ng\">“it builds/it works on my machine”</em>syndrome, by the way, a syndrome that will entirely disappear with containerization.</li>\n<li id=\"0069\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">Canary test:</strong> Test that is automated, non-destructive and that is <strong class=\"mu nv\">run on a regular basis</strong> in a <strong class=\"mu nv\">live</strong> environment, such that if it ever fails, something really bad has happened in reality.</li>\n</ul>\n<p id=\"6c16\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >So, to resume, the toolbox that Python can provide for testing is huge and well-detailed. In a sake for a better understanding, I will stick my examples to <strong class=\"mu nv\">Pytest</strong> in order to exemplify unit testing and test-runs .</p>\n<p id=\"4df1\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">b)</strong> <strong class=\"mu nv\">Analysing code with Python tools</strong></p>\n<p id=\"4891\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Python is an awesome programming language that includes support for almost everything you can image, even linter tools. A <strong class=\"mu nv\">linter</strong> or <strong class=\"mu nv\">lint</strong> refers to tools that analyze source code to flag programming errors, bugs, stylistic errors, and suspicious constructs. The term originates from a UNIX utility that examined C language source code. Lint-like tools generally perform static analysis of source code.</p>\n<p id=\"c92c\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Specifically, in Python, you can use several lint tools, because they all complement each other nicely:</p>\n<ul class=\"\">\n<li id=\"6c42\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf ot ou ov\" ><strong class=\"mu nv\">pycodestyle </strong>(before pep8)<strong class=\"mu nv\">:</strong> for checking the style.</li>\n<li id=\"3dee\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">pyflakes:</strong> for fast static analysis.</li>\n<li id=\"c3b1\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">mccabe:</strong> to find code that is too complex and needs refactoring.</li>\n<li id=\"2bce\" class=\"ms mt dg bt mu b mv ow mx ox mz oy nb oz nd pa nf ot ou ov\" ><strong class=\"mu nv\">pylint:</strong> for everything.</li>\n</ul>\n<p id=\"5e62\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >As this article is about demonstrating a DevSecOps pipeline can be followed with simplicity, I will concentrate in the use of <strong class=\"mu nv\">Pylint</strong> for the static code analysis.</p>\n<p id=\"fe20\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">c)</strong> <strong class=\"mu nv\">Tests in execution: PYTEST</strong></p>\n<p id=\"ba89\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Rather than a tool, Pytest is a framework that makes it easy to write small tests, yet scales to support complex functional testing for applications and libraries. The best part is, the overhead for creating unit tests is close to zero.</p>\n<p id=\"ee84\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Pytest makes use of Python’s “assert” statement to compute assertions. Assertions are a systematic way to check that the internal state of a program is as the programmer expected, with the goal of catching bugs. In particular, they’re good for catching false assumptions that were made while writing the code, or abuse of an interface by another programmer. In addition, they can act as in-line documentation to some extent, by making the programmer’s assumptions obvious.</p>\n<p id=\"3552\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >A simple Pytest check can be as simple as this:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"6f94\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">import unittest<br>from unnecessary_math import multiply</em></span><span id=\"47eb\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">def test_numbers_3_4():<br>    assert(multiply(3,4) == 12)</em></span></pre>\n<p id=\"4d7a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Pytest works having a test folder in your project. Inside this folder, Pytest will look for Python files with the following conventions:</p>\n<ul>\n<li id=\"5a2d\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Files should be named “<em class=\"ng\">test_&lt;function&gt;.py</em>”, being <em class=\"ng\">&lt;function&gt;</em> the name of the python function that you want to (unit) test.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">The unit test inside the test file should be named “<em class=\"ng\">test_&lt;type_of_data&gt;_&lt;function&gt;.py”</em>, being <em class=\"ng\">&lt;type_of_data&gt;</em> the data type and <em class=\"ng\">&lt;function&gt;</em> the name of the function to test.</li>\n</ul>\n<p id=\"ac2b\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >In the tests folder of the “echo” microservice I’ve included a couple of dumb/dummy tests to document the testing process with Pytest.</p>\n<p id=\"8a96\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">== test_index.py ==</strong></p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"4e7f\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">“””<br>This is a test file for pytest<br>“””<br>import pytest<br>from flask import Flask, request<br> <br>TESTAPP = Flask(__name__)<br> <br>@pytest.fixture<br>def test_client():<br>    “”” This sets testing mode “””<br>    TESTAPP.config[‘TESTING’] = True</em></span><span id=\"fe63\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">def test_string_index():<br>    “”” This will test the echo endpoint for a specific behavior”””<br>    with TESTAPP.test_request_context(‘/’):<br>        assert request.path == ‘/’</em></span></pre>\n<p id=\"8b2c\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">== test_echo.py ==</strong></p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"436a\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">“””<br>This is a test file for pytest<br>“””<br>import pytest<br>from flask import Flask, request<br> <br>TESTAPP = Flask(__name__)<br> <br>@pytest.fixture<br>def test_client():<br>    “”” This sets testing mode “””<br>    TESTAPP.config[‘TESTING’] = True<br> <br>def test_string_echo():<br>    “”” This will test the echo endpoint for a specific behavior”””<br>    with TESTAPP.test_request_context(‘/echo/1’):<br>    assert request.path == ‘/echo/1’</em></span></pre>\n<p id=\"aadb\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Running Pytest on the above pieces of code will result in something like this:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"e33b\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">(venv) jon@Laptop ~/DevOps/PythonAPIContainers $ </em><strong class=\"nx nv\"><em class=\"ng\">pytest -vv — disable-warnings</em></strong><em class=\"ng\">ç<br>============= test session starts =============<br>platform darwin — Python 3.7.0, pytest-3.8.1, py-1.6.0, pluggy-0.7.1 –<br>/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7<br>cachedir: .pytest_cache<br>rootdir: /Users/jon/Documents/DevOps/Github/PythonAPIContainers, inifile:</em></span><span id=\"28cf\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">collected 2 items<br>tests/test_echo.py::test_string_echo </em><strong class=\"nx nv\"><em class=\"ng\">PASSED</em></strong><em class=\"ng\"> [ 50%]<br>tests/test_index.py::test_string_index </em><strong class=\"nx nv\"><em class=\"ng\">PASSED</em></strong><em class=\"ng\"> [100%]</em></span><span id=\"7f07\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">========= 2 passed, 3 warnings in 0.31 seconds =========</em></span></pre>\n<p id=\"c6a0\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Notice the two lines in green where Pytest confirms the files in the “tests” folder have been executed and the result has been satisfactory.</p>\n<p id=\"78b0\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >In case the executed tests would fail, the resume would be showing a complete python stack trace with the programming issues found and a final message saying “0 tests passed”.</p>\n<p id=\"30a9\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" ><strong class=\"mu nv\">a)</strong> <strong class=\"mu nv\">Tests in execution: PYLINT</strong></p>\n<p id=\"6e28\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Pylint’s range of features, and the fact that it is highly active and maintained and can be easily automated (with Apycot, Hudson, Travis or Jenkins), make it the absolute must have tool for the job. It supports a number of features, from coding standards to error detection, and it also helps with refactoring by detecting duplicated code.</p>\n<p id=\"88fb\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Pylint is overly pedantic out of the box and benefits from a minimal effort of configuration, but it is fully customizable through a <em class=\"ng\">pylintrc</em> file where you select which errors or conventions are relevant/irrelevant to you. Nonetheless, most messages sent out by the tool will be self-explanatory.</p>\n<p id=\"eb23\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >If we slightly alter the content of the “echo” app.py microservice included in this article before, and perform some alterations on the file to force to have irregularities yet leaving it operational from programming perspective, we can see how Pylint works:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"5b3e\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><strong class=\"nx nv\"><em class=\"ng\">(venv) jon@Laptop ~/DevOps/PythonAPIContainers $ pylint app.py<br>************* Module app<br></em></strong><em class=\"ng\">app.py:135:0: C0304: Final newline missing (missing-final-newline)<br>app.py:28:0: C0111: Missing function docstring (missing-docstring)<br>app.py:41:0: C0111: Missing function docstring (missing-docstring)<br>app.py:54:0: C0111: Missing function docstring (missing-docstring)<br>app.py:69:0: C0111: Missing function docstring (missing-docstring)<br>app.py:74:0: C0111: Missing function docstring (missing-docstring)<br>app.py:81:0: C0111: Missing function docstring (missing-docstring)<br>app.py:100:0: C0111: Missing function docstring (missing-docstring)<br>app.py:12:0: C0411: standard import “from os import environ” should be placed before “import settings” (wrong-import-order)<br>app.py:13:0: C0411: standard import “from datetime import datetime” should be placed before “import settings” (wrong-import-order)<br>app.py:14:0: C0411: third party import “from flask import Flask, jsonify, make_response, url_for, request” should be placed before “import settings” (wrong-import-order)<br> — — — — — — — — — — — — — — — — — -<br></em><strong class=\"nx nv\"><em class=\"ng\">Your code has been rated at 6.76/10</em></strong></span></pre>\n<p id=\"1b81\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The results of Pylint can be interpreted this way:</p>\n<ul>\n<li id=\"1430\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">First section, in maroon color, as the text between brackets indicates, a final blank line needs to be added to the file.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Second section, in purple color, indicates that the docstring is missing for several functions. A docstring is the string literal specified in source code that is used, like a comment, to document a specific segment of code.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Third section, in blue color, indicates the source code is unordered and requires tidiness.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Finally, at the end of the report, in color red, source code is rated on a scale from 0 to 10. This an important value for the developer to check how complaint (s)he is against the PEPs (Python Enhancement Proposals)</li>\n</ul>\n<p id=\"64b1\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >When the code is corrected, Pylint asserts:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"35d2\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><strong class=\"nx nv\"><em class=\"ng\">Your code has been rated at 10.00/10 (previous run: 6.76/10, +3.24)</em></strong></span></pre>\n<p id=\"bc0d\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >On a DevSecOps (and QA) pipeline that means automation of tests (static code analysis) can be fairly simple to address, by not just mentioning the fact the developer should always be interested in coding correctly.</p>\n<h2 id=\"7ab0\" class=\"gk gl dg bt bs di mi nh mk ni mm nj mo nk mq nl go\"><strong class=\"bi\">DOCKERFILE: THE CONTAINERIZATIN PROCCESS</strong></h2>\n<p id=\"5458\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >A <strong class=\"mu nv\">Dockerfile</strong> is a text document that contains all the commands a user could call on the command line to assemble a container image. Using docker-build users can create an automated build that executes several command-line instructions in succession.</p>\n<p id=\"0538\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >An example Dockerfile follows:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"b74f\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">FROM alpine:3.8<br>MAINTAINER Jonathan Gonzalez &lt;j@0x30.io&gt; @EA1HET<br> <br> RUN apk add — no-cache python3 \\<br> &amp;&amp; python3 -m ensurepip \\<br> &amp;&amp; pip3 install -U pip \\<br> &amp;&amp; rm -rf /usr/lib/python*/ensurepip \\<br> &amp;&amp; rm -rf /root/.cache \\<br> &amp;&amp; rm -rf /var/cache/apk/* \\<br> &amp;&amp; find / \\<br> \\( -type d -a -name test -o -name tests \\) \\<br> -o \\( -type f -a -name ‘*.pyc’ -o -name ‘*.pyo’ \\) \\<br> -exec rm -rf ‘{}’ + \\<br> &amp;&amp; mkdir /app<br> <br>EXPOSE 5000<br>WORKDIR /app<br>COPY . /app<br>RUN pip install --no-cache-dir -r requirements.txt<br>ENTRYPOINT /app/boot.sh</em></span></pre>\n<p id=\"c30c\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >The Dockerfile used for the containerization of the “echo” microservice is the one above, but, How it works?</p>\n<ul>\n<li id=\"ca47\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">The first line, in red, defines what is the base content the container uses in order to get built. A base can be a set or libraries or a micro-operating system. In the present case, a micro BusyBox-based system is used to run the Python microservice.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">The second block, in orange, defines the different actions on packages required to prepare the micro BusyBox-based system for running the current version of Python and the microservice written in Python.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">The next line, in violet, indicates which TCP port will be used to expose IP communications from the container to the container orchestrator.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Finally, in green, code is copied from the developer computer to the container image. All the files will be placed in the container image as if it were the on de developer computer, so, if something works on the developer computer, it will work on the container image. A final sentence is included describing which executable file needs to be run when the container image is loaded in memory by the container orchestrator.</li>\n</ul>\n<p id=\"8874\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Now that a Dockerfile is ready, we just need to issue the different commands required to assembre the container image. They are</p>\n<h2 id=\"5544\" class=\"nw gl dg bt bs di pb pc pd pe pf pg ph pi pj pk pl\" >BUILD A CONTAINER IMAGE</h2>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"7a06\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">docker build . -t my_api:v1</em></span></pre>\n<p id=\"95dc\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >where:</p>\n<ul>\n<li id=\"d3d8\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“.” represents the current directory where you are.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“-t” means you want to tag your container image with a compound name.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“my_api” is the name of the container image.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“v1” is the part of the tag name related to the container version.</li>\n</ul>\n<h2 id=\"f4e2\" class=\"nw gl dg bt bs di pb pc pd pe pf pg ph pi pj pk pl\" >RUN A CONTAINER IMAGE</h2>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"1d08\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">docker run -p 5000:5000 my_api:v1 -it api_container</em></span></pre>\n<p id=\"06a9\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >where:</p>\n<ul>\n<li id=\"130a\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“-p 5000:5000” is the TCP port mapped internally and externally to a container.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“my_api:v1” is the local image you want to run as a container.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“-it” runs interactively providing a more readable name to this container in order to clearly identify it when issuing a <em class=\"ng\">“docker ps”</em> command.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“api_container” is the name you want to assign to this container image when it is run.</li>\n</ul>\n<h2 id=\"03c8\" class=\"nw gl dg bt bs di pb pc pd pe pf pg ph pi pj pk pl\" >EXECUTE A SHELL INSIDE THE DOCKER CONTAINER (IF ANY)</h2>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"7944\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">docker exec -it &lt;container_id&gt; ash</em></span></pre>\n<p id=\"8ffa\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >where:</p>\n<ul>\n<li id=\"263e\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“exec” indicates the intention to run a command associated to a container.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“-it &lt;container_id&gt; means the command to execute will be executed in interactive format. A container ID (that can be retrieved issuing a “docker ps” needs to be detailed.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">“ash” is the name of an Alpine Linux shell.</li>\n</ul>\n<p id=\"e7cd\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >Finally, in order to publish a container in a container registry, a few more commands need to be executed on a sequence. They are:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"47ff\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><em class=\"ng\">export DOCKER_ID_USER=”username”</em><em class=\"ng\"><br>docker login<br>docker tag my_api:v1 $DOCKER_ID_USER/my_api:v1<br>docker push $DOCKER_ID_USER/my_api:v1</em></span></pre>\n<ol>\n<li id=\"f97b\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">An env var is defined with the Id of the Docker Registry user that will register the container in the repository.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Next command will perform a login negotiation with the Docker Registry and will validate the user.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">If not done before, the container image will be properly tagged before being uploaded to the Docker Registry.</li>\n<li class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\">Finally, the Docker image is pushed to the Docker Registry with the naming convention defined before.</li>\n</ol>\n<p id=\"0232\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >A log on the above actions is next:</p>\n<pre class=\"lx ly lz ma mb kj iq dx\"><span id=\"5493\" class=\"nw gl dg bt nx b fo ny nz s oa\" ><strong class=\"nx nv\"><em class=\"ng\">jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $ docker build . -t my_api:v1<br></em></strong><em class=\"ng\">Sending build context to Docker daemon  50.18kB<br>Step 1/8 : FROM alpine:3.8<br>3.8: Pulling from library/alpine<br>4fe2ade4980c: Pull complete<br>Digest: sha256:621c2f39f8133acb8e64023a94dbdf0d5ca81896102b9e57c0dc184cadaf5528<br>Status: Downloaded newer image for alpine:3.8<br>---&gt; 196d12cf6ab1<br>Step 2/8 : MAINTAINER Jonathan Gonzalez &lt;j@0x30.io&gt; @EA1HET<br>---&gt; Running in 9e3607810efb<br>Removing intermediate container 9e3607810efb<br>---&gt; 85c02539f662<br>Step 3/8 : RUN apk add --no-cache python3                                      &amp;&amp; python3 -m ensurepip                                         &amp;&amp; pip3 install -U pip                                          &amp;&amp; rm -rf /usr/lib/python*/ensurepip                            &amp;&amp; rm -rf /root/.cache                                          &amp;&amp; rm -rf /var/cache/apk/*                                      &amp;&amp; find /                                                           \\( -type d -a -name test -o -name tests \\)                      -o \\( -type f -a -name '*.pyc' -o -name '*.pyo' \\)              -exec rm -rf '{}' +                                         &amp;&amp; mkdir /app<br>---&gt; Running in ef7cb26a0cf9<br>fetch </em><a href=\"http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz\" class=\"bc co nr ns nt nu\"><em class=\"ng\">http://dl-cdn.alpinelinux.org/alpine/v3.8/main/x86_64/APKINDEX.tar.gz</em></a><em class=\"ng\"><br>fetch </em><a href=\"http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz\" class=\"bc co nr ns nt nu\"><em class=\"ng\">http://dl-cdn.alpinelinux.org/alpine/v3.8/community/x86_64/APKINDEX.tar.gz</em></a><em class=\"ng\"><br>(1/11) Installing libbz2 (1.0.6-r6)<br>(2/11) Installing expat (2.2.5-r0)<br>(3/11) Installing libffi (3.2.1-r4)<br>(4/11) Installing gdbm (1.13-r1)<br>(5/11) Installing xz-libs (5.2.4-r0)<br>(6/11) Installing ncurses-terminfo-base (6.1_p20180818-r1)<br>(7/11) Installing ncurses-terminfo (6.1_p20180818-r1)<br>(8/11) Installing ncurses-libs (6.1_p20180818-r1)<br>(9/11) Installing readline (7.0.003-r0)(10/11) Installing sqlite-libs (3.24.0-r0)<br>(11/11) Installing python3 (3.6.6-r0)<br>Executing busybox-1.28.4-r1.trigger<br>OK: 67 MiB in 24 packages<br>Looking in links: /tmp/tmpngqpumos<br>Requirement already satisfied: setuptools in /usr/lib/python3.6/site-packages (39.0.1)<br>Requirement already satisfied: pip in /usr/lib/python3.6/site-packages (10.0.1)<br>Collecting pip<br>Downloading https://files.pythonhosted.org/packages/5f/25/e52d3f31441505a5f3af41213346e5b6c221c9e086a166f3703d2ddaf940/pip-18.0-py2.py3-none-any.whl (1.3MB)<br>Installing collected packages: pip<br>Found existing installation: pip 10.0.1<br>Uninstalling pip-10.0.1:<br>Successfully uninstalled pip-10.0.1<br>Successfully installed pip-18.0<br>Removing intermediate container ef7cb26a0cf9<br>---&gt; e578960ef329<br>Step 4/8 : EXPOSE 5000<br>---&gt; Running in 533208720ceb<br>Removing intermediate container 533208720ceb<br>---&gt; 889a718aae42<br>Step 5/8 : WORKDIR /app<br>---&gt; Running in 2ab712110618<br>Removing intermediate container 2ab712110618<br>---&gt; b6fe8d313419<br>Step 6/8 : COPY . /app<br>---&gt; 7eb793bbe089<br>Step 7/8 : RUN pip install --no-cache-dir -r requirements.txt<br>---&gt; Running in ae95eb68de95<br>Collecting aniso8601==3.0.2 (from -r requirements.txt (line 1))<br>Downloading </em><a href=\"https://files.pythonhosted.org/packages/17/13/eecdcc638c0ea3b105ebb62ff4e76914a744ef1b6f308651dbed368c6c01/aniso8601-3.0.2-py2.py3-none-any.whl\" class=\"bc co nr ns nt nu\"><em class=\"ng\">https://files.pythonhosted.org/packages/17/13/eecdcc638c0ea3b105ebb62ff4e76914a744ef1b6f308651dbed368c6c01/aniso8601-3.0.2-py2.py3-none-any.whl</em></a><em class=\"ng\"><br>Collecting click==6.7 (from -r requirements.txt (line 2))<br>Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)<br>Collecting environs==4.0.0 (from -r requirements.txt (line 3))<br>Downloading </em><a href=\"https://files.pythonhosted.org/packages/64/19/c1b8df73d2b2e4c704e65e1ec1423714f10cf2bf5489e7dac724eda62218/environs-4.0.0-py2.py3-none-any.whl\" class=\"bc co nr ns nt nu\"><em class=\"ng\">https://files.pythonhosted.org/packages/64/19/c1b8df73d2b2e4c704e65e1ec1423714f10cf2bf5489e7dac724eda62218/environs-4.0.0-py2.py3-none-any.whl</em></a><em class=\"ng\"><br>Collecting Flask==1.0.2 (from -r requirements.txt (line 4))<br>Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)<br>Collecting gunicorn==19.9.0 (from -r requirements.txt (line 5))<br>Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)<br>Collecting itsdangerous==0.24 (from -r requirements.txt (line 6))<br>Downloading https://files.pythonhosted.org/packages/dc/b4/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4/itsdangerous-0.24.tar.gz (46kB)<br>Collecting Jinja2==2.10 (from -r requirements.txt (line 7))<br>Downloading https://files.pythonhosted.org/packages/7f/ff/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731/Jinja2-2.10-py2.py3-none-any.whl (126kB)<br>Collecting MarkupSafe==1.0 (from -r requirements.txt (line 8))<br>Downloading </em><a href=\"https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz\" class=\"bc co nr ns nt nu\"><em class=\"ng\">https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz</em></a><em class=\"ng\"><br>Collecting marshmallow==2.15.6 (from -r requirements.txt (line 9))<br>Downloading https://files.pythonhosted.org/packages/3f/4d/cb555dfc2e2f926179884665fa1e6ae6b8f8102e4f8228b73e2a30eb0ee0/marshmallow-2.15.6-py2.py3-none-any.whl (44kB)<br>Collecting python-dotenv==0.9.1 (from -r requirements.txt (line 10))<br>Downloading </em><a href=\"https://files.pythonhosted.org/packages/24/3d/977140bd94bfb160f98a5c02fdfbb72325130f12a325cf993182956e9d0e/python_dotenv-0.9.1-py2.py3-none-any.whl\" class=\"bc co nr ns nt nu\"><em class=\"ng\">https://files.pythonhosted.org/packages/24/3d/977140bd94bfb160f98a5c02fdfbb72325130f12a325cf993182956e9d0e/python_dotenv-0.9.1-py2.py3-none-any.whl</em></a><em class=\"ng\"><br>Collecting pytz==2018.5 (from -r requirements.txt (line 11))<br>Downloading https://files.pythonhosted.org/packages/30/4e/27c34b62430286c6d59177a0842ed90dc789ce5d1ed740887653b898779a/pytz-2018.5-py2.py3-none-any.whl (510kB)<br>Collecting Werkzeug==0.14.1 (from -r requirements.txt (line 12))<br>Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)<br>Installing collected packages: aniso8601, click, python-dotenv, marshmallow, environs, itsdangerous, Werkzeug, MarkupSafe, Jinja2, Flask, gunicorn, pytz<br>Running setup.py install for itsdangerous: started<br>Running setup.py install for itsdangerous: finished with status 'done'<br>Running setup.py install for MarkupSafe: started<br>Running setup.py install for MarkupSafe: finished with status 'done'<br>Successfully installed Flask-1.0.2 Jinja2-2.10 MarkupSafe-1.0 Werkzeug-0.14.1 aniso8601-3.0.2 click-6.7 environs-4.0.0 gunicorn-19.9.0 itsdangerous-0.24 marshmallow-2.15.6 python-dotenv-0.9.1 pytz-2018.5<br>Removing intermediate container ae95eb68de95<br>---&gt; 4e18577c050b<br>Step 8/8 : ENTRYPOINT /app/boot.sh<br>---&gt; Running in 9006f33efa75<br>Removing intermediate container 9006f33efa75<br>---&gt; 59ad5a3902a3<br></em><strong class=\"nx nv\"><em class=\"ng\">Successfully built 59ad5a3902a3<br>Successfully tagged my_api:v1</em></strong></span><span id=\"a835\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><strong class=\"nx nv\"><em class=\"ng\">jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $ docker image ls<br></em></strong><em class=\"ng\">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<br></em><strong class=\"nx nv\"><em class=\"ng\">my_api              v1                  59ad5a3902a3        6 seconds ago       43.2MB<br></em></strong><em class=\"ng\">alpine              3.8                 196d12cf6ab1        13 days ago         4.41MB</em></span><span id=\"7f71\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $<br>jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $<br></em><strong class=\"nx nv\"><em class=\"ng\">jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $ export DOCKER_ID_USER=\"ea1het\"<br>jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $ docker login</em></strong></span><span id=\"985e\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><em class=\"ng\">Authenticating with existing credentials...<br>Login Succeeded</em></span><span id=\"c04d\" class=\"nw gl dg bt nx b fo ob oc od oe of nz s oa\" ><strong class=\"nx nv\"><em class=\"ng\">jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $ docker tag my_api:v1 $DOCKER_ID_USER/my_api:v1<br>jon@MiniHET ~/Documents/DevOps/Github/PythonAPIContainers $ docker push $DOCKER_ID_USER/my_api:v1<br>The push refers to repository [docker.io/ea1het/my_api]<br>578236ec6f44: Pushed<br>f4bab11cc8bc: Pushed<br>58e68352ccbd: Pushed<br>df64d3292fd6: Mounted from library/alpine<br>v1: digest: sha256:be0cbe0fd7290ac017745473fafabf611eaaf026f6a5d5b5fbd7a21a6fa7754f size: 1158</em></strong></span></pre>\n<p id=\"b9be\" class=\"ms mt dg bt mu b mv nm mx nn mz no nb np nd nq nf\" >And this is the reflection of the above action on the Docker Hub (the public Docker Registry):</p>\n<p ><img src=\"https://miro.medium.com/max/850/1*QLhuuf-wijXfO3-FlgWvBA.png\" alt=\"Docker Hub stores public images of containers\" width=\"425\" height=\"317\"></p>\n<p>Docker Hub stores public images of containers</p>\n<p> </p>\n<p><img src=\"https://miro.medium.com/max/850/1*KXf0iKuqNb0W4FcQKsc-WQ.png\" alt=\"Detail on the container creation. Pay attention to the badges at the bottom left\" width=\"425\" height=\"235\"></p>\n<p>Detail on the container creation. Pay attention to the badges at the bottom left</p>\n<p> </p>\n<p><img src=\"https://miro.medium.com/max/854/1*YxWymOTw4GMkLqCWz8T6tg.png\" alt=\"Size detail on the content recently created\" width=\"427\" height=\"228\"></p>\n<p> Size detail on the content recently created</p>\n<p> </p>\n<p><img src=\"https://miro.medium.com/max/850/1*EuTSq7g4KqQUej80-jm4AA.png\" alt=\"Metadata detail on container composition\" width=\"425\" height=\"407\"></p>\n<p id=\"mcetoc_1di1aq6cm0\">Metadata detail on container composition</p>\n<h2 id=\"mcetoc_1di1c1v2o0\">Colophon</h2>\n<p id=\"62f8\" class=\"ms mt dg bt mu b mv mw mx my mz na nb nc nd ne nf\" >Writing this article was an interesting experience. Writing for newcomers is difficult, more than for experienced professionals, but, this is the more rewarding, in my humble opinion. Hope this article served you.</p>",
            "image": "https://0x30.io/media/posts/11/1_wVrgeExYzpDWBma8g6q5TA.png",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2018-09-28T23:43:00+02:00",
            "date_modified": "2019-08-12T00:17:46+02:00"
        },
        {
            "id": "https://0x30.io/microservices-101/",
            "url": "https://0x30.io/microservices-101/",
            "title": "Microservices 101",
            "summary": "Microservice is a term that started to be really hot in 2014. Companies around the&hellip;",
            "content_html": "<p>Microservice is a term that started to be really hot in 2014. Companies around the world and software engineers are paying since then special attention to this paradigm as a new way to think about structuring applications.</p>\n<p><strong>In short, microservices relates to a way to modularize big applications, so, each microservice is independent in the form and autonomous in the type of execution compared to the others. The inter-service communication is always using standards, like HTTP, and use to have the operational form of RESTful services or messaging buses:</strong></p>\n<ul>\n<li><strong>The REST paradigm is quite close to the microservice paradigm.</strong></li>\n<li><strong>The messaging buses are not ESB-like, where all application logic is on the bus, but more event oriented buses, where the microservice owns the business logic and uses events to warn other microservices actions should be taken.</strong></li>\n</ul>\n<h3 id=\"mcetoc_1di1cjipk0\"><strong>The original microservice concept.</strong></h3>\n<p>In their original whitepaper, the authors explain that \"t<em>he microservice architectural style is an approach to developing a single application as a </em><strong><em>suite of small services</em></strong><em>, each </em><strong><em>running in its own process</em></strong><em> and communicating with lightweight mechanisms, often an HTTP resource API. These services are </em><strong><em>built around business capabilities</em></strong><em> and </em><strong><em>independently deployable</em></strong><em> by fully automated deployment machinery. There is a </em><strong><em>bare minimum of centralized management</em></strong><em>of these services, which may be written in different programming languages and use different data storage technologies\" (J. Lewis, M. Fowler).</em></p>\n<p>But, what does all means really? It means this new paradigm, a philosophy of work, a group of technologies and a set of best practices, for instance being used since lately 2000, involves software that should be created to achieve several precepts by design alongside with their business objectives:</p>\n<ul>\n<li><u>Componentization via Services:</u> No more libraries, no more bundles, no more packages. Each microservice is what its name represents, a service. Microservices does not relate to the size of the application inside but to the way they behave in group. Microservices have single responsibility, are loosely coupled and are highly cohesive. That means, each microservice serves for one concept or (business) objective. A highly cohesive component encapsulates one concept. Simply speaking, it does one thing well. Highly cohesive modules are easier to understand, easier to maintain, and easier to reuse. The idea of loose coupling relates to how tightly the behaviour of one component is bound to the implementation details of other components. As you will read later, the database is not anything else than an implementation detail and cannot drive the work of any microservice. Simple evolution is the final objective.</li>\n<li><u>Organized around Business Capabilities:</u> This refers to the fact that each microservice is created to solve one single business objective. Teams working on microservices are multidisciplinary, so, e.g. tech guys, functional guys and business guys work together to provide the correct vision to what the business needs. In a tech jargon, teams working on microservices are typically \"two pizza teams\", a definition coming from Agile development and coined by Amazon, meaning that the teams are not bigger than 6 or 8 people working per microservice at the time.</li>\n<li><u>Products not Projects:</u> This point is difficult to explain on a general way as it encompasses the company idiosyncrasy, people likes and dislikes and common behaviour per industry, but, in general it refers to the fact that the team that designs a microservices is the same team that builds it, the same team that evolve the service and that run operations. In this case, when a team needs to deal with a microservice they see the code as a complete product, rather than just a project we need to face temporarily.</li>\n<li><u>Smart endpoints and dumb pipes:</u> In short, do not create ESB (Enterprise Service Bus) anymore. The business logic must reside in the microservice, not in a common line of communication. This creates bottlenecks and inefficiencies. The organization reflects what their software is (M. Conway's law).</li>\n<li><u>Decentralized Governance:</u> There's no more architects that rules the software building process. Architects should look after standards usage and platform being operational, but should not force the development teams to use specifics like language or persistence models. A decentralized governance produce anti-fragility and this anti-fragility produces agility and speed.</li>\n<li><u>Decentralized Data Management:</u> Each microservice has its own database, or even better, each microservice has its own implementation details. There's no more integration by database, this practice is from the past. There's no more canonical database model. No more database tables full of null fields per row. Databases are implementation details.</li>\n<li><u>Infrastructure Automation:</u> Infrastructure as code. Automation is a must to be able to work with microservices. They are a lot, and operations are quite difficult if certain degree of automation is not in place.</li>\n<li><u>Design for failure:</u> You must think microservices are made to fail, as like the technological ecosystem fails too, but, this fact is something that makes the microservice paradigm strong, as you already are aware of that and build infrastructures that are redundant, and follow best practices to get business continuity, thus you are resilient by design. Following this behaviour, you will never lose the 100% of your system, or you won't lose the full system functionalities, just only some of them. This is the cloud way of life.</li>\n<li><u>Evolutionary Design:</u> You are not forced to build the definite service that will work forever. You can start writing code to solve that part of the business need you best known and slowly introduce changes in the microservice as you know better what you are facing. You don't need months to study a certain situation, you can act quickly and evolve with the microservice as the real need evolves. That's awesome, because you can start providing something useful for the business since moment zero.</li>\n</ul>\n<h3 id=\"mcetoc_1di1cjipk1\"><strong>The good, the bad.</strong></h3>\n<p>Every decision comes with trade-offs. Microservices couldn't be less:</p>\n<ul>\n<li><u>PROS</u>: Microservices reinforce modular structure, which is particularly important for larger teams. Simple services are easier to deploy, and since they are autonomous, are less likely to cause system failures when they go wrong. With microservices you can mix multiple languages, development frameworks and data-storage technologies.</li>\n<li><u>CONS</u>: Distributed systems are harder to program, since remote calls are slow and are always at risk of failure. Maintaining strong consistency is extremely difficult for a distributed system, which means everyone has to manage eventual consistency. You need a mature operations team to manage lots of services, which are being redeployed regularly.</li>\n</ul>\n<h3 id=\"mcetoc_1di1cjipk2\"><strong>Revolve on the thing</strong></h3>\n<p>A couple of simple yet repetitive in happening examples that illustrate common situations we all face while architecting a software solution:</p>\n<ul>\n<li>Several micro applications running alone but attacking a common database is not microservices. These could be more similar to a SOA architecture. Despite it can be useful, it is not microservices. In microservices each microservice has its own logic and the use of a database is an implementation detail only. The important is to provide and API style, a way to provide data, or request it, to/from other microservices. The way the information is stored should not be important. Moreover, microservices can be able to change data storage, and business logic, internally and without affecting other microservices.</li>\n<li>Microservices communicating among them not using REST is possible, but it will be much more difficult its broad integration into the microservice ecosystem. The use of HTTP is important to take advantage of everything already in place for the HTTP standard, like caches, load balancers, proxies, .... and the like.</li>\n</ul>\n<h3 id=\"mcetoc_1di1cjipk3\"><strong>Colophon</strong></h3>\n<p><em>“While our experiences so far are positive compared to monolithic applications, we're conscious of the fact that not enough time has passed for us to make a full judgement.”</em></p>\n<p><em>-- James Lewis and Martin Fowler</em></p>",
            "image": "https://0x30.io/media/posts/13/microservices.png",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2017-08-20T00:26:00+02:00",
            "date_modified": "2019-08-12T00:29:45+02:00"
        },
        {
            "id": "https://0x30.io/data-ingestion-fluentd-or-logstash-thats-the-question/",
            "url": "https://0x30.io/data-ingestion-fluentd-or-logstash-thats-the-question/",
            "title": "Data ingestion: Fluentd or Logstash. That&#x27;s the question",
            "summary": "IntroductionWhile preparing a stack for a project this summer I came across the decision to&hellip;",
            "content_html": "<h2 id=\"mcetoc_1di1c7f000\"><strong>Introduction</strong></h2>\n<p>While preparing a stack for a project this summer I came across the decision to use a data ingestion tool and, after some days analysing different tools, the last two on the table were <a href=\"https://www.fluentd.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Fluentd</strong></a> and <a href=\"https://www.elastic.co/products/logstash\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Logstash</strong></a>. So, I decided to research a bit and do my tests. Following is the result of that work with both tools.</p>\n<h2 id=\"mcetoc_1di1c7f001\"><strong>A bit of history and factual data</strong></h2>\n<p>Logstash is well known for being part of the <a href=\"https://www.elastic.co/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>ELK</strong></a> (Elasticsearch, Logstash, Kibana) stack. Fluentd is built by <a href=\"https://www.treasuredata.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Treasure Data</a> and is part of the <a href=\"https://www.cncf.io/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Cloud Native Computing Foundation (CNCF)</a> portfolio of tools that are increasingly used by many renamed DevOps-oriented communities like <a href=\"https://docs.docker.com/reference/logging/fluentd/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Docker</strong></a>, <a href=\"https://github.com/GoogleCloudPlatform/google-fluentd\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Google Cloud Platform (GCP)</strong></a> and even Elasticsearch.</p>\n<p><u>If you plan on using Elastic products</u> or the whole suite, then <u>you should tend to prefer Logstash</u> (although Fluentd also has excellent support for Elastic). On the other hand, <u>if you’re using any CNCF hosted project (e.g. Kubernetes, OpenTracing or Prometheus), you should probably go with Fluentd </u>prefereably.</p>\n<h2 id=\"mcetoc_1di1c7f002\"><strong>Some brief technical stuff to mention and not to forget</strong></h2>\n<ul>\n<li>Both platforms can run either on Linux or Windows.</li>\n<li>Both can be configured to function in a multi-tier setup.</li>\n<li>In both cases forwarders can detect failure of a shipper and switch to another active shipper when necessary. A data shipper is a software unit in charge of automating the backup of a database and transaction data or log files on a primary production server onto a secondary or standby server. A data forwarder is a vehicle that carries data from the origin point to a destination point.</li>\n<li>Both deliver events to many different receivers, for instance readily supporting SQS, Elasticsearch, and S3.</li>\n<li>Both handle JSON events natively.</li>\n<li>Both are under active development.</li>\n<li>Both use a plugin architecture and have their own plugin manager tools.</li>\n<li>Event data on Logstash uses a single stream and then uses algorithmic if-then statements to send them to the right destination. Fluentd relies on tags to route events; each tag tells Fluentd where it wants to be routed. Fluentd’s approach is more declarative whereas Logstash’s method is procedural.</li>\n</ul>\n<p><strong><em>Update</em></strong><em> (by Aaron Mildenstein, Software Engineer at Elastic)</em><strong><em>:</em></strong><em> In Logstash 6.0 the product will fully support multiple, independent pipelines which can be stopped and started, and automatically reloaded on the fly.</em></p>\n<ul>\n<li>Plugins are managed differntly by both tools. In the case of Logstash it uses a centralized repository in GitHub whereas Fluentd is based on a decentralized model. Both have hundreds of plugins available to almost everything you can get data from: application logs, network protocols, IoT devices, container technologies, databases, orchestration engines, message protocols, mail services, monitoring tools.... just to name a few.</li>\n<li>Transport technology is where the big differences appear in plain. When we talk about transport technology we are referring to the act and fact of gather data from disparate sources and \"transport this data\" to the correct destinations (a database, a data lake, another application, an API or whatever it should be). Logstash lacks an internal persistent mechanism. Currently, it has an on-memory queue that holds a determined number of events and relies on an external message queue system like Redis, Kafka, ZeroMQ or RabbitMQ for persistence across restarts and scalability. For me this is centainly an issue for Logstash, and the community has been requesting for long to persist the queue on-disk. Fluentd, on the other hand, has a highly configurable buffering system. It can be either in-memory or on-disk, thus, in comparison to Logstash we can say that Fluentd has built-in reliability and scalability characteristics. The downside with Fluentd is that its configuration might be difficult or even tricky for newbies. Beside this, there's another few things to bear in mind. Beats, the agent-like tool from Elastic sends data to Logstash with a minimal filtering capabilities, thus, it sends over the network pretty much everything it records. Fluent Bit, or Fluentd Forwarder, offers some extended filtering capabilities to reduce the amount of data sent over the network. For many that wouldn't be important enough, except for those who have their data infrastructure deployed on hybrid or public clouds. Every bit send or received counts and it has a price.</li>\n</ul>\n<p><strong><em>Update</em></strong><em> (by Aaron Mildenstein, Software Engineer at Elastic)</em><strong><em>:</em></strong><em> The persistent-queue feature has allowed persistence to disk for several point versions (in beta release form), and is now out of beta and fully supported. You can choose whether to use persist-to-disk, or the in-memory queue.</em></p>\n<ul>\n<li>In terms of support, unfortunately if I'm not wrong it seems that Logstash does not offer enterprise grade support for Logstash <em>per se</em>. Fluentd, on the other hand, does support enterprises.</li>\n</ul>\n<h2 id=\"mcetoc_1di1c7f003\"><strong>Some notes about performance</strong></h2>\n<p>This is a grey area of discussion and I cannot provide more deep details than those already provided by the community of users. Asking colleagues on this point I came across the fact that Fluentd has a slightly better reputation when it comes to performance. In any case, from my personal point of view, the true thing is that both tools performed really well with high workloads. But, How high can they perform under the same tech specs?. This is some factual data I could gather:</p>\n<ul>\n<li>Logstash consumes around 100-150 MB of RAM; Fluentd consumes around 30-70 MB of RAM. For modern hardware infrastructure this is ridiculous, but the difference between applications when you are deploying a whole datacentre might become tens, hundreds or thousands of additional RAM gigabytes that must be paid, so, despite it is a hardly meaningful technical fact, we should not forget never any tech spec or fact.</li>\n<li>For small machines, or IoT devices, Logstash uses Elastic Beats, a miniaturized agent to perform a set of the product capabilities. Same happens with Fluentd, that can deploy Fluentd Bit and Fluentd Forwarder for the same task.</li>\n<li>In my tests I came accross some slowliness on Logstash, compared to Fluentd but the difference is imperceptible in the scales we usually work.</li>\n</ul>\n<h2 id=\"mcetoc_1di1c7f004\">Conclusion</h2>\n<p>Use either Logstash or Fluentd will depend on user experience and punctual project needs. We cannot remove from the picture the technology bias as well. That being said, Fluentd is written mostly in Ruby, with performance-sensitive parts written in C, and with a more convenient, pre-compiled stable version available. Fluentd also has a forwarder written in Go language, that provides an excelent performance. Logstash's forwarder is written in Go language too, while its shipper runs on JRuby, which requires the JVM. I don't like this, despite some people say that the JVM is a nice bless and hell if you are able to configure it well, but personally, I understand this is a serious overkill for a data shipper.</p>\n<p>Finally, in terms of architecture, I feel myself more comfortable with Fluentd's pluggable architecture and built-in reliability and scalability features than with Logstash capabilities. The unified logging to JSON is another feature I like thinking on micro services and APIs, so, you can unify all facets of processing data: collecting, filtering, buffering, and outputting data across multiple sources and destinations.</p>\n<p><strong><u>My actual bet, on a generic project, is for Fluentd.</u></strong></p>\n<p>Best regards <em>(and thank you Aaron, for the udpate on Logstash)</em></p>\n<h2 id=\"mcetoc_1di1c8p7c5\">ARTICLE UPDATE:</h2>\n<div id=\"ember199\" class=\"comments-comment-item__post-meta feed-shared-post-meta is-comment feed-shared-post-meta--is-not-sponsored ember-view\">\n<p id=\"mcetoc_1di1c9pjm6\" class=\"feed-shared-post-meta__actor  t-12 t-black--light t-normal\"><span class=\" feed-shared-post-meta__name t-14 t-black t-bold\"><span class=\"hoverable-link-text\" >According to Aaron Mildenstein, P</span></span><span class=\"feed-shared-post-meta__headline t-12 t-black--light t-normal\">rincipal Consulting Architect at Elastic</span><span id=\"ember210\" class=\"ember-view\">Event, the fact that in the past data on Logstash needed to use a single stream and then uses algorithmic if-then statements to send them to the right destination will no longer be necessary for event routing starting on </span><span id=\"ember210\" class=\"ember-view\">Logstash 6.0. </span><span id=\"ember210\" class=\"ember-view\">It will fully support multiple, independent pipelines which can be stopped and started, and automatically reloaded on the fly. </span></p>\n<p class=\"feed-shared-post-meta__actor  t-12 t-black--light t-normal\"><span id=\"ember210\" class=\"ember-view\">In the same sense, it also necessary to say that the persistent-queue feature has allowed persistence to disk for several point versions (in beta release form). You can choose whether to use persist-to-disk, or the in-memory queue. </span></p>\n<p class=\"feed-shared-post-meta__actor  t-12 t-black--light t-normal\"><span id=\"ember210\" class=\"ember-view\">Finally, Logstash support is offered by Elastic as part of their support contract offerings which cover all Elastic products. It is accurate to state, however, that Elastic does not offer support for Logstash by itself.</span></p>\n</div>\n<div class=\"comments-comment-item-content-body\"> </div>",
            "image": "https://0x30.io/media/posts/12/1.jpeg",
            "author": {
                "name": "Jonathan Gonzalez"
            },
            "tags": [
            ],
            "date_published": "2017-08-15T00:24:00+02:00",
            "date_modified": "2019-08-12T00:27:41+02:00"
        }
    ]
}
